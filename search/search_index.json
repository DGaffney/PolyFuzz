{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PolyFuzz \u00b6 PolyFuzz performs fuzzy string matching, string grouping, and contains extensive evaluation functions. PolyFuzz is meant to bring fuzzy string matching techniques together within a single framework. Currently, methods include Levenshtein distance with RapidFuzz, a character-based n-gram TF-IDF, word embedding techniques such as FastText and GloVe, and \ud83e\udd17 transformers embeddings. The philosophy of PolyFuzz is: Easy to use yet highly customizable . It is a string matcher tool that requires only a few lines of code but that allows you customize and create your own models.","title":"Home"},{"location":"#polyfuzz","text":"PolyFuzz performs fuzzy string matching, string grouping, and contains extensive evaluation functions. PolyFuzz is meant to bring fuzzy string matching techniques together within a single framework. Currently, methods include Levenshtein distance with RapidFuzz, a character-based n-gram TF-IDF, word embedding techniques such as FastText and GloVe, and \ud83e\udd17 transformers embeddings. The philosophy of PolyFuzz is: Easy to use yet highly customizable . It is a string matcher tool that requires only a few lines of code but that allows you customize and create your own models.","title":"PolyFuzz"},{"location":"releases/","text":"v0.4.0 \u00b6 Added new models (SentenceTransformers, Gensim, USE, Spacy) Added .fit , .transform , and .fit_transform methods Added .save and PolyFuzz.load() SentenceTransformers from polyfuzz.models import SentenceEmbeddings distance_model = SentenceEmbeddings(\"all-MiniLM-L6-v2\") model = PolyFuzz(distance_model) Gensim from polyfuzz.models import GensimEmbeddings distance_model = GensimEmbeddings(\"glove-twitter-25\") model = PolyFuzz(distance_model) USE from polyfuzz.models import USEEmbeddings distance_model = USEEmbeddings(\"https://tfhub.dev/google/universal-sentence-encoder/4\") model = PolyFuzz(distance_model) Spacy from polyfuzz.models import SpacyEmbeddings distance_model = SpacyEmbeddings(\"en_core_web_md\") model = PolyFuzz(distance_model) fit, transform, fit_transform Add fit , transform , and fit_transform in order to use PolyFuzz in production #34 from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer from polyfuzz import PolyFuzz train_words = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] unseen_words = [\"apple\", \"apples\", \"mouse\"] # Fit model = PolyFuzz(\"TF-IDF\") model.fit(train_words) # Transform results = model.transform(unseen_words) In the code above, we fit our TF-IDF model on train_words and use .transform() to match the words in unseen_words to the words that we trained on in train_words . After fitting our model, we can save it as follows: model.save(\"my_model\") Then, we can load our model to be used elsewhere: from polyfuzz import PolyFuzz model = PolyFuzz.load(\"my_model\") v0.3.4 \u00b6 Make sure that when you use two lists that are exactly the same, it will return 1 for identical terms: from polyfuzz import PolyFuzz from_list = [\"apple\", \"house\"] model = PolyFuzz(\"TF-IDF\") model.match(from_list, from_list) This will match each word in from_list to itself and give it a score of 1. Thus, apple will be matched to apple and house will be mapped to house . However, if you input just a single list, it will try to map them within the list without mapping to itself: from polyfuzz import PolyFuzz from_list = [\"apple\", \"apples\"] model = PolyFuzz(\"TF-IDF\") model.match(from_list) In the example above, apple will be mapped to apples and not to apple . Here, we assume that the user wants to find the most similar words within a list without mapping to itself. v0.3.3 \u00b6 Update numpy to \"numpy>=1.20.0\" to prevent this and this issue Update pytorch to \"torch>=1.4.0,<1.7.1\" to prevent save_state_warning error v0.3.2 \u00b6 Fix exploding memory usage when using top_n v0.3.0 \u00b6 Use top_n in polyfuzz.models.TFIDF and polyfuzz.models.Embeddings v0.2.2 \u00b6 Update grouping to include all strings only if identical lists of strings are compared v0.2.0 \u00b6 Update naming convention matcher --> model Update documentation Add basic models to grouper Fix issues with vector order in cosine similarity Update naming of cosine similarity function v0.1.0 \u00b6 Additional tests More thorough documentation Prepare for public release v0.0.1 \u00b6 First release of PolyFuzz Matching through: Edit Distance TF-IDF Embeddings Custom models Grouping of results with custom models Evaluation through precision-recall curves","title":"Releases"},{"location":"releases/#v040","text":"Added new models (SentenceTransformers, Gensim, USE, Spacy) Added .fit , .transform , and .fit_transform methods Added .save and PolyFuzz.load() SentenceTransformers from polyfuzz.models import SentenceEmbeddings distance_model = SentenceEmbeddings(\"all-MiniLM-L6-v2\") model = PolyFuzz(distance_model) Gensim from polyfuzz.models import GensimEmbeddings distance_model = GensimEmbeddings(\"glove-twitter-25\") model = PolyFuzz(distance_model) USE from polyfuzz.models import USEEmbeddings distance_model = USEEmbeddings(\"https://tfhub.dev/google/universal-sentence-encoder/4\") model = PolyFuzz(distance_model) Spacy from polyfuzz.models import SpacyEmbeddings distance_model = SpacyEmbeddings(\"en_core_web_md\") model = PolyFuzz(distance_model) fit, transform, fit_transform Add fit , transform , and fit_transform in order to use PolyFuzz in production #34 from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer from polyfuzz import PolyFuzz train_words = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] unseen_words = [\"apple\", \"apples\", \"mouse\"] # Fit model = PolyFuzz(\"TF-IDF\") model.fit(train_words) # Transform results = model.transform(unseen_words) In the code above, we fit our TF-IDF model on train_words and use .transform() to match the words in unseen_words to the words that we trained on in train_words . After fitting our model, we can save it as follows: model.save(\"my_model\") Then, we can load our model to be used elsewhere: from polyfuzz import PolyFuzz model = PolyFuzz.load(\"my_model\")","title":"v0.4.0"},{"location":"releases/#v034","text":"Make sure that when you use two lists that are exactly the same, it will return 1 for identical terms: from polyfuzz import PolyFuzz from_list = [\"apple\", \"house\"] model = PolyFuzz(\"TF-IDF\") model.match(from_list, from_list) This will match each word in from_list to itself and give it a score of 1. Thus, apple will be matched to apple and house will be mapped to house . However, if you input just a single list, it will try to map them within the list without mapping to itself: from polyfuzz import PolyFuzz from_list = [\"apple\", \"apples\"] model = PolyFuzz(\"TF-IDF\") model.match(from_list) In the example above, apple will be mapped to apples and not to apple . Here, we assume that the user wants to find the most similar words within a list without mapping to itself.","title":"v0.3.4"},{"location":"releases/#v033","text":"Update numpy to \"numpy>=1.20.0\" to prevent this and this issue Update pytorch to \"torch>=1.4.0,<1.7.1\" to prevent save_state_warning error","title":"v0.3.3"},{"location":"releases/#v032","text":"Fix exploding memory usage when using top_n","title":"v0.3.2"},{"location":"releases/#v030","text":"Use top_n in polyfuzz.models.TFIDF and polyfuzz.models.Embeddings","title":"v0.3.0"},{"location":"releases/#v022","text":"Update grouping to include all strings only if identical lists of strings are compared","title":"v0.2.2"},{"location":"releases/#v020","text":"Update naming convention matcher --> model Update documentation Add basic models to grouper Fix issues with vector order in cosine similarity Update naming of cosine similarity function","title":"v0.2.0"},{"location":"releases/#v010","text":"Additional tests More thorough documentation Prepare for public release","title":"v0.1.0"},{"location":"releases/#v001","text":"First release of PolyFuzz Matching through: Edit Distance TF-IDF Embeddings Custom models Grouping of results with custom models Evaluation through precision-recall curves","title":"v0.0.1"},{"location":"api/linkage/","text":"polyfuzz.linkage \u00b6 single_linkage ( matches , min_similarity = 0.8 ) \u00b6 Show source code in polyfuzz\\linkage.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def single_linkage ( matches : pd . DataFrame , min_similarity : float = 0.8 ) -> Tuple [ Mapping [ int , List [ str ]], Mapping [ str , int ], Mapping [ str , str ]]: \"\"\" Single linkage clustering from column 'From' to column 'To' `matches` contains three columns: *From*, *To*, and *Similarity* where *Similarity* is already the minimum similarity score and thus no checking for minimum similarity is necessary. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for creating groups min_similarity: minimum similarity between strings before they can be merged into a group Returns: clusters: The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster \"\"\" matches = matches . loc [ matches . Similarity > min_similarity , :] cluster_mapping = {} cluster_id = 0 for row in matches . itertuples (): # If from string has not already been mapped if not cluster_mapping . get ( row . From ): # If the to string has not already been mapped if not cluster_mapping . get ( row . To ): cluster_mapping [ row . To ] = cluster_id cluster_mapping [ row . From ] = cluster_id cluster_id += 1 # If the to string has already been mapped else : cluster_mapping [ row . From ] = cluster_mapping . get ( row . To ) # Populate the clusters clusters = {} for key , value in cluster_mapping . items (): clusters . setdefault ( value , []) clusters [ value ] . append ( key ) cluster_name_map = { key : clusters . get ( value )[ 0 ] for key , value in cluster_mapping . items ()} return clusters , cluster_mapping , cluster_name_map Single linkage clustering from column 'From' to column 'To' matches contains three columns: From , To , and Similarity where Similarity is already the minimum similarity score and thus no checking for minimum similarity is necessary. Parameters Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for creating groups required min_similarity float minimum similarity between strings before they can be merged into a group 0.8 Returns Type Description Tuple[Mapping[int, List[str]], Mapping[str, int], Mapping[str, str]] clusters: The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster","title":"Linkage"},{"location":"api/linkage/#polyfuzzlinkage","text":"","title":"polyfuzz.linkage"},{"location":"api/linkage/#polyfuzz.linkage.single_linkage","text":"Show source code in polyfuzz\\linkage.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def single_linkage ( matches : pd . DataFrame , min_similarity : float = 0.8 ) -> Tuple [ Mapping [ int , List [ str ]], Mapping [ str , int ], Mapping [ str , str ]]: \"\"\" Single linkage clustering from column 'From' to column 'To' `matches` contains three columns: *From*, *To*, and *Similarity* where *Similarity* is already the minimum similarity score and thus no checking for minimum similarity is necessary. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for creating groups min_similarity: minimum similarity between strings before they can be merged into a group Returns: clusters: The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster \"\"\" matches = matches . loc [ matches . Similarity > min_similarity , :] cluster_mapping = {} cluster_id = 0 for row in matches . itertuples (): # If from string has not already been mapped if not cluster_mapping . get ( row . From ): # If the to string has not already been mapped if not cluster_mapping . get ( row . To ): cluster_mapping [ row . To ] = cluster_id cluster_mapping [ row . From ] = cluster_id cluster_id += 1 # If the to string has already been mapped else : cluster_mapping [ row . From ] = cluster_mapping . get ( row . To ) # Populate the clusters clusters = {} for key , value in cluster_mapping . items (): clusters . setdefault ( value , []) clusters [ value ] . append ( key ) cluster_name_map = { key : clusters . get ( value )[ 0 ] for key , value in cluster_mapping . items ()} return clusters , cluster_mapping , cluster_name_map Single linkage clustering from column 'From' to column 'To' matches contains three columns: From , To , and Similarity where Similarity is already the minimum similarity score and thus no checking for minimum similarity is necessary. Parameters Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for creating groups required min_similarity float minimum similarity between strings before they can be merged into a group 0.8 Returns Type Description Tuple[Mapping[int, List[str]], Mapping[str, int], Mapping[str, str]] clusters: The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster","title":"single_linkage()"},{"location":"api/metrics/","text":"polyfuzz.metrics \u00b6 precision_recall_curve ( matches , precision_steps = 0.01 ) \u00b6 Show source code in polyfuzz\\metrics.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def precision_recall_curve ( matches : pd . DataFrame , precision_steps : float = 0.01 ) -> Tuple [ List [ float ], List [ float ], List [ float ]]: \"\"\" Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision precision_steps: the incremental steps in minimum precision Returns: min_precisions: minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step \"\"\" min_precisions = list ( np . arange ( 0. , 1 + precision_steps , precision_steps )) average_precision = [] recall = [] similarities = matches . Similarity . values total = len ( matches ) for min_precision in min_precisions : selection = similarities [ similarities >= min_precision ] recall . append ( len ( selection ) / total ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = RuntimeWarning ) average_precision . append ( float ( np . mean ( selection ))) return min_precisions , recall , average_precision Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for calculating precision, recall, and average precision required precision_steps float the incremental steps in minimum precision 0.01 Returns Type Description Tuple[List[float], List[float], List[float]] min_precisions: minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step visualize_precision_recall ( matches , min_precisions , recall , kde = True , save_path = None ) \u00b6 Show source code in polyfuzz\\metrics.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def visualize_precision_recall ( matches : Mapping [ str , pd . DataFrame ], min_precisions : Mapping [ str , List [ float ]], recall : Mapping [ str , List [ float ]], kde : bool = True , save_path : str = None ): \"\"\" Visualize the precision recall curve for one or more models Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision per model min_precisions: minimum precision steps per model recall: recall per minimum precision step per model kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python visualize_precision_recall(matches, min_precisions, recall, save_path=\"data/results.png\") ``` \"\"\" SMALL_SIZE = 10 MEDIUM_SIZE = 12 BIGGER_SIZE = 14 plt . rc ( 'font' , size = SMALL_SIZE ) # controls default text sizes plt . rc ( 'axes' , titlesize = SMALL_SIZE ) # fontsize of the axes title plt . rc ( 'axes' , labelsize = MEDIUM_SIZE ) # fontsize of the x and y labels plt . rc ( 'xtick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'ytick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'legend' , fontsize = SMALL_SIZE ) # legend fontsize plt . rc ( 'figure' , titlesize = BIGGER_SIZE ) # fontsize of the figure title if not isinstance ( matches , dict ): matches = { \"Model\" : matches } min_precisions = { \"Model\" : min_precisions } recall = { \"Model\" : recall } # Create single dataset of similarity score for all models distribution_data = [( matches [ name ] . Similarity . values , [ name for _ in range ( len ( matches [ name ]))]) for name in matches . keys ()] distribution_data = pd . DataFrame ( np . hstack ( distribution_data ) . T , columns = [ \"Similarity\" , \"Model\" ]) distribution_data . Similarity = distribution_data . Similarity . astype ( float ) model_names = list ( matches . keys ()) # Create layout cmap = get_cmap ( 'Accent' ) fig = plt . figure ( figsize = ( 20 , 5 )) if len ( model_names ) == 1 : middle = 0 else : middle = .1 if kde : widths = [ 1.5 , middle , 1.5 ] else : widths = [ 1.5 , middle , 0 ] heights = [ 1.5 ] gs = gridspec . GridSpec ( 1 , 3 , width_ratios = widths , height_ratios = heights ) ax1 = plt . subplot ( gs [:, 0 ]) if kde : ax2 = plt . subplot ( gs [:, 2 ], sharex = ax1 ) # Precision-recall curve for color , model_name in zip ( cmap . colors , model_names ): ax1 . plot ( min_precisions [ model_name ], recall [ model_name ], color = color ) ax1 . set_ylim ( bottom = 0 , top = 1 ) ax1 . set_xlim ( left = 0 , right = 1 ) ax1 . spines [ 'right' ] . set_visible ( False ) ax1 . spines [ 'top' ] . set_visible ( False ) ax1 . set_xlabel ( r \"$\\bf {Precision} $\" + \" \\n (Minimum Similarity)\" ) ax1 . set_ylabel ( r \"$\\bf {Recall} $\" + \" \\n (Percentage Matched)\" ) # Similarity Histogram if kde : for color , model_name in zip ( cmap . colors , model_names ): sns . kdeplot ( matches [ model_name ][ \"Similarity\" ], fill = True , ax = ax2 , color = color ) ax2 . yaxis . set_label_position ( \"right\" ) ax2 . yaxis . tick_right () ax2 . set_xlabel ( r \"$\\bf {Similarity} $\" ) ax2 . set_ylabel ( \"\" ) ax2 . set_xlim ( left =- 0 , right = 1 ) plt . setp ([ ax2 ], title = 'Score Frequency - KDE' ) # Titles if len ( model_names ) == 1 and kde : fig . suptitle ( f 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) elif kde : fig . suptitle ( 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) else : fig . suptitle ( 'Precision-Recall Curve' , size = 20 , y = 1 , x = 0.45 ) # Custom Legend if len ( model_names ) > 1 : custom_lines = [ Line2D ([ 0 ], [ 0 ], color = color , lw = 4 ) for color , model_name in zip ( cmap . colors , model_names )] ax1 . legend ( custom_lines , model_names , bbox_to_anchor = ( 1.05 , .61 , .7 , .902 ), loc = 3 , ncol = 1 , borderaxespad = 0. , frameon = True , fontsize = 10 ) if save_path : plt . savefig ( save_path , dpi = 300 ) Visualize the precision recall curve for one or more models Parameters Name Type Description Default matches Mapping[str, pandas.core.frame.DataFrame] contains the columns From , To , and Similarity used for calculating precision, recall, and average precision per model required min_precisions Mapping[str, List[float]] minimum precision steps per model required recall Mapping[str, List[float]] recall per minimum precision step per model required kde bool whether to also visualize the kde plot True save_path str the path to save the resulting image to None Usage: visualize_precision_recall ( matches , min_precisions , recall , save_path = \"data/results.png\" )","title":"Metrics"},{"location":"api/metrics/#polyfuzzmetrics","text":"","title":"polyfuzz.metrics"},{"location":"api/metrics/#polyfuzz.metrics.precision_recall_curve","text":"Show source code in polyfuzz\\metrics.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def precision_recall_curve ( matches : pd . DataFrame , precision_steps : float = 0.01 ) -> Tuple [ List [ float ], List [ float ], List [ float ]]: \"\"\" Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision precision_steps: the incremental steps in minimum precision Returns: min_precisions: minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step \"\"\" min_precisions = list ( np . arange ( 0. , 1 + precision_steps , precision_steps )) average_precision = [] recall = [] similarities = matches . Similarity . values total = len ( matches ) for min_precision in min_precisions : selection = similarities [ similarities >= min_precision ] recall . append ( len ( selection ) / total ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = RuntimeWarning ) average_precision . append ( float ( np . mean ( selection ))) return min_precisions , recall , average_precision Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for calculating precision, recall, and average precision required precision_steps float the incremental steps in minimum precision 0.01 Returns Type Description Tuple[List[float], List[float], List[float]] min_precisions: minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step","title":"precision_recall_curve()"},{"location":"api/metrics/#polyfuzz.metrics.visualize_precision_recall","text":"Show source code in polyfuzz\\metrics.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def visualize_precision_recall ( matches : Mapping [ str , pd . DataFrame ], min_precisions : Mapping [ str , List [ float ]], recall : Mapping [ str , List [ float ]], kde : bool = True , save_path : str = None ): \"\"\" Visualize the precision recall curve for one or more models Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision per model min_precisions: minimum precision steps per model recall: recall per minimum precision step per model kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python visualize_precision_recall(matches, min_precisions, recall, save_path=\"data/results.png\") ``` \"\"\" SMALL_SIZE = 10 MEDIUM_SIZE = 12 BIGGER_SIZE = 14 plt . rc ( 'font' , size = SMALL_SIZE ) # controls default text sizes plt . rc ( 'axes' , titlesize = SMALL_SIZE ) # fontsize of the axes title plt . rc ( 'axes' , labelsize = MEDIUM_SIZE ) # fontsize of the x and y labels plt . rc ( 'xtick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'ytick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'legend' , fontsize = SMALL_SIZE ) # legend fontsize plt . rc ( 'figure' , titlesize = BIGGER_SIZE ) # fontsize of the figure title if not isinstance ( matches , dict ): matches = { \"Model\" : matches } min_precisions = { \"Model\" : min_precisions } recall = { \"Model\" : recall } # Create single dataset of similarity score for all models distribution_data = [( matches [ name ] . Similarity . values , [ name for _ in range ( len ( matches [ name ]))]) for name in matches . keys ()] distribution_data = pd . DataFrame ( np . hstack ( distribution_data ) . T , columns = [ \"Similarity\" , \"Model\" ]) distribution_data . Similarity = distribution_data . Similarity . astype ( float ) model_names = list ( matches . keys ()) # Create layout cmap = get_cmap ( 'Accent' ) fig = plt . figure ( figsize = ( 20 , 5 )) if len ( model_names ) == 1 : middle = 0 else : middle = .1 if kde : widths = [ 1.5 , middle , 1.5 ] else : widths = [ 1.5 , middle , 0 ] heights = [ 1.5 ] gs = gridspec . GridSpec ( 1 , 3 , width_ratios = widths , height_ratios = heights ) ax1 = plt . subplot ( gs [:, 0 ]) if kde : ax2 = plt . subplot ( gs [:, 2 ], sharex = ax1 ) # Precision-recall curve for color , model_name in zip ( cmap . colors , model_names ): ax1 . plot ( min_precisions [ model_name ], recall [ model_name ], color = color ) ax1 . set_ylim ( bottom = 0 , top = 1 ) ax1 . set_xlim ( left = 0 , right = 1 ) ax1 . spines [ 'right' ] . set_visible ( False ) ax1 . spines [ 'top' ] . set_visible ( False ) ax1 . set_xlabel ( r \"$\\bf {Precision} $\" + \" \\n (Minimum Similarity)\" ) ax1 . set_ylabel ( r \"$\\bf {Recall} $\" + \" \\n (Percentage Matched)\" ) # Similarity Histogram if kde : for color , model_name in zip ( cmap . colors , model_names ): sns . kdeplot ( matches [ model_name ][ \"Similarity\" ], fill = True , ax = ax2 , color = color ) ax2 . yaxis . set_label_position ( \"right\" ) ax2 . yaxis . tick_right () ax2 . set_xlabel ( r \"$\\bf {Similarity} $\" ) ax2 . set_ylabel ( \"\" ) ax2 . set_xlim ( left =- 0 , right = 1 ) plt . setp ([ ax2 ], title = 'Score Frequency - KDE' ) # Titles if len ( model_names ) == 1 and kde : fig . suptitle ( f 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) elif kde : fig . suptitle ( 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) else : fig . suptitle ( 'Precision-Recall Curve' , size = 20 , y = 1 , x = 0.45 ) # Custom Legend if len ( model_names ) > 1 : custom_lines = [ Line2D ([ 0 ], [ 0 ], color = color , lw = 4 ) for color , model_name in zip ( cmap . colors , model_names )] ax1 . legend ( custom_lines , model_names , bbox_to_anchor = ( 1.05 , .61 , .7 , .902 ), loc = 3 , ncol = 1 , borderaxespad = 0. , frameon = True , fontsize = 10 ) if save_path : plt . savefig ( save_path , dpi = 300 ) Visualize the precision recall curve for one or more models Parameters Name Type Description Default matches Mapping[str, pandas.core.frame.DataFrame] contains the columns From , To , and Similarity used for calculating precision, recall, and average precision per model required min_precisions Mapping[str, List[float]] minimum precision steps per model required recall Mapping[str, List[float]] recall per minimum precision step per model required kde bool whether to also visualize the kde plot True save_path str the path to save the resulting image to None Usage: visualize_precision_recall ( matches , min_precisions , recall , save_path = \"data/results.png\" )","title":"visualize_precision_recall()"},{"location":"api/polyfuzz/","text":"polyfuzz.polyfuzz.PolyFuzz \u00b6 PolyFuzz class for Fuzzy string matching, grouping, and evaluation. Parameters Name Type Description Default method Union[str, polyfuzz.models._base.BaseMatcher, List[polyfuzz.models._base.BaseMatcher]] the method(s) used for matching. For quick selection of models select one of the following: \"EditDistance\", \"TF-IDF\" or \"Embeddings\". If you want more control over the models above, pass in a model from polyfuzz.models. For examples, see usage below. 'TF-IDF' verbose bool Changes the verbosity of the model, Set to True if you want to track the stages of the model. False Usage: For basic, out-of-the-box usage, run the code below. You can replace \"TF-IDF\" with either \"EditDistance\" or \"Embeddings\" for quick access to these models: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" ) If you want more control over the String Matching models, you can load in these models separately: tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , model_id = \"TF-IDF-Sklearn\" ) model = pf . PolyFuzz ( tfidf ) You can also select multiple models in order to compare performance: tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , model_id = \"TF-IDF-Sklearn\" ) edit = EditDistance ( n_jobs =- 1 ) model = pf . PolyFuzz ([ tfidf , edit ]) You can use embedding model, like Flair: from flair.embeddings import WordEmbeddings , TransformerWordEmbeddings fasttext_embedding = WordEmbeddings ( 'news' ) bert_embedding = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) embedding = Embeddings ([ fasttext_embedding , bert_embedding ], min_similarity = 0.0 ) model = pf . PolyFuzz ( embedding ) fit ( self , from_list , to_list = None ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def fit ( self , from_list : List [ str ], to_list : List [ str ] = None ): \"\"\" Fit one or model distance models on `from_list` if no `to_list` is given or fit them on `to_list` if both `from_list` and `to_list` are given. Typically, the `to_list` will be tracked as the list that we want to transform our `from_list` to. In other words, it is the golden list of words that we want the words in the `from_list` mapped to. However, you can also choose a single `from_list` and leave `to_list` empty to map all words from within `from_list` to each other. Then, `from_list` will be tracked instead as the golden list of words. Thus, if you want to train on a single list instead, use only `from_list` and keep `to_list` empty. Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` Now, whenever you apply `.transform(new_list)`, the `new_list` will be mapped to the words in `to_list`. You can also fit on a single list of words: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit([\"string_three\", \"string_four\"]) ``` \"\"\" self . match ( from_list , to_list ) if to_list is not None : self . to_list = to_list else : self . to_list = from_list return self Fit one or model distance models on from_list if no to_list is given or fit them on to_list if both from_list and to_list are given. Typically, the to_list will be tracked as the list that we want to transform our from_list to. In other words, it is the golden list of words that we want the words in the from_list mapped to. However, you can also choose a single from_list and leave to_list empty to map all words from within from_list to each other. Then, from_list will be tracked instead as the golden list of words. Thus, if you want to train on a single list instead, use only from_list and keep to_list empty. Parameters Name Type Description Default from_list List[str] The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the from_list and ignore the to_list . required to_list List[str] The list where you want to map to None Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . fit ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) Now, whenever you apply .transform(new_list) , the new_list will be mapped to the words in to_list . You can also fit on a single list of words: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . fit ([ \"string_three\" , \"string_four\" ]) fit_transform ( self , from_list , to_list = None ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def fit_transform ( self , from_list : List [ str ], to_list : List [ str ] = None ) -> Mapping [ str , pd . DataFrame ]: \"\"\" Fit and transform lists of words on one or more distance models. Typically, the `to_list` will be tracked as the list that we want to transform our `from_list` to. In other words, it is the golden list of words that we want the words in the `from_list` mapped to. However, you can also choose a single `from_list` and leave `to_list` empty to map all words from within `from_list` to each other. Then, `from_list` will be tracked instead as the golden list of words. Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") results = model.fit_transform(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can also fit and transform a single list of words: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") results = model.fit_transform([\"string_three\", \"string_four\"]) ``` \"\"\" self . fit ( from_list , to_list ) return self . transform ( from_list ) Fit and transform lists of words on one or more distance models. Typically, the to_list will be tracked as the list that we want to transform our from_list to. In other words, it is the golden list of words that we want the words in the from_list mapped to. However, you can also choose a single from_list and leave to_list empty to map all words from within from_list to each other. Then, from_list will be tracked instead as the golden list of words. Parameters Name Type Description Default from_list List[str] The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the from_list and ignore the to_list . required to_list List[str] The list where you want to map to None Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) results = model . fit_transform ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) You can also fit and transform a single list of words: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) results = model . fit_transform ([ \"string_three\" , \"string_four\" ]) get_cluster_mappings ( self , name = None ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 416 417 418 419 420 421 422 423 424 425 426 427 def get_cluster_mappings ( self , name : str = None ) -> Mapping [ str , int ]: \"\"\" Get the mappings from the `To` column to its respective column \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . cluster_mappings . values ())[ 0 ] elif len ( self . matches ) > 1 and name : return self . cluster_mappings [ name ] return self . cluster_mappings Get the mappings from the To column to its respective column get_clusters ( self , model_id = None ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def get_clusters ( self , model_id : str = None ) -> Mapping [ str , List [ str ]]: \"\"\" Get the groupings/clusters from a single model Arguments: model_id: the model id of the model if you have specified multiple models \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . clusters . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . clusters [ model_id ] return self . clusters Get the groupings/clusters from a single model Parameters Name Type Description Default model_id str the model id of the model if you have specified multiple models None get_ids ( self ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 375 376 377 378 379 380 381 382 383 def get_ids ( self ) -> Union [ str , List [ str ], None ]: \"\"\" Get all model ids for easier access \"\"\" check_matches ( self ) if isinstance ( self . method , str ): return self . method elif isinstance ( self . method , Iterable ): return [ model . model_id for model in self . method ] return None Get all model ids for easier access get_matches ( self , model_id = None ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 385 386 387 388 389 390 391 392 393 394 395 396 def get_matches ( self , model_id : str = None ) -> Union [ pd . DataFrame , Mapping [ str , pd . DataFrame ]]: \"\"\" Get the matches from one or more models\"\"\" check_matches ( self ) if len ( self . matches ) == 1 : return list ( self . matches . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . matches [ model_id ] return self . matches Get the matches from one or more models group ( self , model = None , link_min_similarity = 0.75 , group_all_strings = False ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 def group ( self , model : Union [ str , BaseMatcher ] = None , link_min_similarity : float = 0.75 , group_all_strings : bool = False ): \"\"\" From the matches, group the `To` matches together using single linkage Arguments: model: you can choose one of the models in `polyfuzz.models` to be used as a grouper link_min_similarity: the minimum similarity between strings before they are grouped in a single linkage fashion group_all_strings: if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. Updates: self.matches: Adds a column `Group` that is the grouped version of the `To` column \"\"\" check_matches ( self ) self . clusters = {} self . cluster_mappings = {} # Standard models - quick access if isinstance ( model , str ): if model in [ \"TF-IDF\" , \"TFIDF\" ]: model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: model = RapidFuzz () elif self . method in [ \"Embeddings\" , \"Embedding\" ]: model = Embeddings ( min_similarity = link_min_similarity ) else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" \"* Or None if you want to automatically use TF-IDF\" ) # Use TF-IDF if no model is specified elif not model : model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) # Group per model for name , match in self . matches . items (): self . _create_groups ( name , model , link_min_similarity , group_all_strings ) From the matches, group the To matches together using single linkage Parameters Name Type Description Default model Union[str, polyfuzz.models._base.BaseMatcher] you can choose one of the models in polyfuzz.models to be used as a grouper None link_min_similarity float the minimum similarity between strings before they are grouped in a single linkage fashion 0.75 group_all_strings bool if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. False Updates: self.matches: Adds a column Group that is the grouped version of the To column load ( path ) (classmethod) \u00b6 Show source code in polyfuzz\\polyfuzz.py 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 @classmethod def load ( cls , path : str ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the PolyFuzz file you want to load Usage: ```python PolyFuzz.load(\"my_model\") ``` \"\"\" with open ( path , 'rb' ) as file : model = joblib . load ( file ) return model Loads the model from the specified path Parameters Name Type Description Default path str the location and name of the PolyFuzz file you want to load required Usage: PolyFuzz . load ( \"my_model\" ) match ( self , from_list , to_list = None , top_n = 1 ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , top_n : int = 1 ): \"\"\" Match the from_list of strings to the to_list of strings with whatever models you have initialized Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to top_n: The number of matches you want returned. This is currently only implemented for `polyfuzz.models.TFIDF` and `polyfuzz.models.Embeddings` as they can computationally handle more comparisons. Updates: self.matches: A dictionary with the matches from all models, can be accessed with `model.get_all_matches` or `model.get_match(\"TF-IDF\")` Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can access the results matches with `model.get_all_matches` or a specific model with `model.get_match(\"TF-IDF\")` based on their model_id. \"\"\" # Standard models - quick access if isinstance ( self . method , str ): if self . method in [ \"TF-IDF\" , \"TFIDF\" ]: self . method = TFIDF ( min_similarity = 0 , top_n = top_n ) self . matches = { \"TF-IDF\" : self . method . match ( from_list , to_list )} elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: self . method = RapidFuzz () self . matches = { \"EditDistance\" : self . method . match ( from_list , to_list )} elif self . method in [ \"Embeddings\" , \"Embedding\" ]: self . method = Embeddings ( min_similarity = 0 , top_n = top_n ) self . matches = { \"Embeddings\" : self . method . match ( from_list , to_list )} else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" ) logger . info ( f \"Ran model with model id = { self . method } \" ) # Custom models elif isinstance ( self . method , BaseMatcher ): self . matches = { self . method . model_id : self . method . match ( from_list , to_list )} logging . info ( f \"Ran model with model id = { self . method . model_id } \" ) # Multiple custom models elif isinstance ( self . method , Iterable ): self . _update_model_ids () self . matches = {} for model in self . method : self . matches [ model . model_id ] = model . match ( from_list , to_list ) logging . info ( f \"Ran model with model id = { model . model_id } \" ) return self Match the from_list of strings to the to_list of strings with whatever models you have initialized Parameters Name Type Description Default from_list List[str] The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the from_list and ignore the to_list . required to_list List[str] The list where you want to map to None top_n int The number of matches you want returned. This is currently only implemented for polyfuzz.models.TFIDF and polyfuzz.models.Embeddings as they can computationally handle more comparisons. 1 Updates: self.matches: A dictionary with the matches from all models, can be accessed with model.get_all_matches or model.get_match(\"TF-IDF\") Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . match ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) You can access the results matches with model.get_all_matches or a specific model with model.get_match(\"TF-IDF\") based on their model_id. save ( self , path ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 429 430 431 432 433 434 435 436 437 438 439 440 441 def save ( self , path : str ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save Usage: ```python model.save(\"my_model\") ``` \"\"\" with open ( path , 'wb' ) as file : joblib . dump ( self , file ) Saves the model to the specified path Parameters Name Type Description Default path str the location and name of the file you want to save required Usage: model . save ( \"my_model\" ) transform ( self , from_list ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def transform ( self , from_list : List [ str ]) -> Mapping [ str , pd . DataFrame ]: \"\"\" After fitting your model, match all words in `from_list` to the words that were fitted on previously. Arguments: from_list: The list from which you want mappings. Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit([\"input_string_1\", \"input_string2\"]) ``` Then, you can transform and normalize new strings: ```python results = model.transform([\"input_string_1\", \"input_string2\"]) ``` \"\"\" all_matches = {} if isinstance ( self . method , BaseMatcher ): matches = self . method . match ( from_list , self . to_list , re_train = False ) all_matches [ self . method . type ] = matches elif isinstance ( self . method , Iterable ): for model in self . method : all_matches [ model . type ] = model . match ( from_list , self . to_list , re_train = False ) return all_matches After fitting your model, match all words in from_list to the words that were fitted on previously. Parameters Name Type Description Default from_list List[str] The list from which you want mappings. required Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . fit ([ \"input_string_1\" , \"input_string2\" ]) Then, you can transform and normalize new strings: results = model . transform ([ \"input_string_1\" , \"input_string2\" ]) visualize_precision_recall ( self , kde = False , save_path = None ) \u00b6 Show source code in polyfuzz\\polyfuzz.py 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 def visualize_precision_recall ( self , kde : bool = False , save_path : str = None ): \"\"\" Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) model.visualize_precision_recall(save_path=\"results.png\") ``` \"\"\" check_matches ( self ) self . min_precisions = {} self . recalls = {} self . average_precisions = {} for name , match in self . matches . items (): min_precision , recall , average_precision = precision_recall_curve ( match ) self . min_precisions [ name ] = min_precision self . recalls [ name ] = recall self . average_precisions [ name ] = average_precision visualize_precision_recall ( self . matches , self . min_precisions , self . recalls , kde , save_path ) Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters Name Type Description Default kde bool whether to also visualize the kde plot False save_path str the path to save the resulting image to None Usage: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . match ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) model . visualize_precision_recall ( save_path = \"results.png\" )","title":"PolyFuzz"},{"location":"api/polyfuzz/#polyfuzzpolyfuzzpolyfuzz","text":"PolyFuzz class for Fuzzy string matching, grouping, and evaluation. Parameters Name Type Description Default method Union[str, polyfuzz.models._base.BaseMatcher, List[polyfuzz.models._base.BaseMatcher]] the method(s) used for matching. For quick selection of models select one of the following: \"EditDistance\", \"TF-IDF\" or \"Embeddings\". If you want more control over the models above, pass in a model from polyfuzz.models. For examples, see usage below. 'TF-IDF' verbose bool Changes the verbosity of the model, Set to True if you want to track the stages of the model. False Usage: For basic, out-of-the-box usage, run the code below. You can replace \"TF-IDF\" with either \"EditDistance\" or \"Embeddings\" for quick access to these models: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" ) If you want more control over the String Matching models, you can load in these models separately: tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , model_id = \"TF-IDF-Sklearn\" ) model = pf . PolyFuzz ( tfidf ) You can also select multiple models in order to compare performance: tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , model_id = \"TF-IDF-Sklearn\" ) edit = EditDistance ( n_jobs =- 1 ) model = pf . PolyFuzz ([ tfidf , edit ]) You can use embedding model, like Flair: from flair.embeddings import WordEmbeddings , TransformerWordEmbeddings fasttext_embedding = WordEmbeddings ( 'news' ) bert_embedding = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) embedding = Embeddings ([ fasttext_embedding , bert_embedding ], min_similarity = 0.0 ) model = pf . PolyFuzz ( embedding )","title":"polyfuzz.polyfuzz.PolyFuzz"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.fit","text":"Show source code in polyfuzz\\polyfuzz.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def fit ( self , from_list : List [ str ], to_list : List [ str ] = None ): \"\"\" Fit one or model distance models on `from_list` if no `to_list` is given or fit them on `to_list` if both `from_list` and `to_list` are given. Typically, the `to_list` will be tracked as the list that we want to transform our `from_list` to. In other words, it is the golden list of words that we want the words in the `from_list` mapped to. However, you can also choose a single `from_list` and leave `to_list` empty to map all words from within `from_list` to each other. Then, `from_list` will be tracked instead as the golden list of words. Thus, if you want to train on a single list instead, use only `from_list` and keep `to_list` empty. Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` Now, whenever you apply `.transform(new_list)`, the `new_list` will be mapped to the words in `to_list`. You can also fit on a single list of words: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit([\"string_three\", \"string_four\"]) ``` \"\"\" self . match ( from_list , to_list ) if to_list is not None : self . to_list = to_list else : self . to_list = from_list return self Fit one or model distance models on from_list if no to_list is given or fit them on to_list if both from_list and to_list are given. Typically, the to_list will be tracked as the list that we want to transform our from_list to. In other words, it is the golden list of words that we want the words in the from_list mapped to. However, you can also choose a single from_list and leave to_list empty to map all words from within from_list to each other. Then, from_list will be tracked instead as the golden list of words. Thus, if you want to train on a single list instead, use only from_list and keep to_list empty. Parameters Name Type Description Default from_list List[str] The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the from_list and ignore the to_list . required to_list List[str] The list where you want to map to None Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . fit ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) Now, whenever you apply .transform(new_list) , the new_list will be mapped to the words in to_list . You can also fit on a single list of words: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . fit ([ \"string_three\" , \"string_four\" ])","title":"fit()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.fit_transform","text":"Show source code in polyfuzz\\polyfuzz.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def fit_transform ( self , from_list : List [ str ], to_list : List [ str ] = None ) -> Mapping [ str , pd . DataFrame ]: \"\"\" Fit and transform lists of words on one or more distance models. Typically, the `to_list` will be tracked as the list that we want to transform our `from_list` to. In other words, it is the golden list of words that we want the words in the `from_list` mapped to. However, you can also choose a single `from_list` and leave `to_list` empty to map all words from within `from_list` to each other. Then, `from_list` will be tracked instead as the golden list of words. Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") results = model.fit_transform(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can also fit and transform a single list of words: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") results = model.fit_transform([\"string_three\", \"string_four\"]) ``` \"\"\" self . fit ( from_list , to_list ) return self . transform ( from_list ) Fit and transform lists of words on one or more distance models. Typically, the to_list will be tracked as the list that we want to transform our from_list to. In other words, it is the golden list of words that we want the words in the from_list mapped to. However, you can also choose a single from_list and leave to_list empty to map all words from within from_list to each other. Then, from_list will be tracked instead as the golden list of words. Parameters Name Type Description Default from_list List[str] The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the from_list and ignore the to_list . required to_list List[str] The list where you want to map to None Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) results = model . fit_transform ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) You can also fit and transform a single list of words: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) results = model . fit_transform ([ \"string_three\" , \"string_four\" ])","title":"fit_transform()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_cluster_mappings","text":"Show source code in polyfuzz\\polyfuzz.py 416 417 418 419 420 421 422 423 424 425 426 427 def get_cluster_mappings ( self , name : str = None ) -> Mapping [ str , int ]: \"\"\" Get the mappings from the `To` column to its respective column \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . cluster_mappings . values ())[ 0 ] elif len ( self . matches ) > 1 and name : return self . cluster_mappings [ name ] return self . cluster_mappings Get the mappings from the To column to its respective column","title":"get_cluster_mappings()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_clusters","text":"Show source code in polyfuzz\\polyfuzz.py 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def get_clusters ( self , model_id : str = None ) -> Mapping [ str , List [ str ]]: \"\"\" Get the groupings/clusters from a single model Arguments: model_id: the model id of the model if you have specified multiple models \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . clusters . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . clusters [ model_id ] return self . clusters Get the groupings/clusters from a single model Parameters Name Type Description Default model_id str the model id of the model if you have specified multiple models None","title":"get_clusters()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_ids","text":"Show source code in polyfuzz\\polyfuzz.py 375 376 377 378 379 380 381 382 383 def get_ids ( self ) -> Union [ str , List [ str ], None ]: \"\"\" Get all model ids for easier access \"\"\" check_matches ( self ) if isinstance ( self . method , str ): return self . method elif isinstance ( self . method , Iterable ): return [ model . model_id for model in self . method ] return None Get all model ids for easier access","title":"get_ids()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_matches","text":"Show source code in polyfuzz\\polyfuzz.py 385 386 387 388 389 390 391 392 393 394 395 396 def get_matches ( self , model_id : str = None ) -> Union [ pd . DataFrame , Mapping [ str , pd . DataFrame ]]: \"\"\" Get the matches from one or more models\"\"\" check_matches ( self ) if len ( self . matches ) == 1 : return list ( self . matches . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . matches [ model_id ] return self . matches Get the matches from one or more models","title":"get_matches()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.group","text":"Show source code in polyfuzz\\polyfuzz.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 def group ( self , model : Union [ str , BaseMatcher ] = None , link_min_similarity : float = 0.75 , group_all_strings : bool = False ): \"\"\" From the matches, group the `To` matches together using single linkage Arguments: model: you can choose one of the models in `polyfuzz.models` to be used as a grouper link_min_similarity: the minimum similarity between strings before they are grouped in a single linkage fashion group_all_strings: if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. Updates: self.matches: Adds a column `Group` that is the grouped version of the `To` column \"\"\" check_matches ( self ) self . clusters = {} self . cluster_mappings = {} # Standard models - quick access if isinstance ( model , str ): if model in [ \"TF-IDF\" , \"TFIDF\" ]: model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: model = RapidFuzz () elif self . method in [ \"Embeddings\" , \"Embedding\" ]: model = Embeddings ( min_similarity = link_min_similarity ) else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" \"* Or None if you want to automatically use TF-IDF\" ) # Use TF-IDF if no model is specified elif not model : model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) # Group per model for name , match in self . matches . items (): self . _create_groups ( name , model , link_min_similarity , group_all_strings ) From the matches, group the To matches together using single linkage Parameters Name Type Description Default model Union[str, polyfuzz.models._base.BaseMatcher] you can choose one of the models in polyfuzz.models to be used as a grouper None link_min_similarity float the minimum similarity between strings before they are grouped in a single linkage fashion 0.75 group_all_strings bool if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. False Updates: self.matches: Adds a column Group that is the grouped version of the To column","title":"group()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.load","text":"Show source code in polyfuzz\\polyfuzz.py 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 @classmethod def load ( cls , path : str ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the PolyFuzz file you want to load Usage: ```python PolyFuzz.load(\"my_model\") ``` \"\"\" with open ( path , 'rb' ) as file : model = joblib . load ( file ) return model Loads the model from the specified path Parameters Name Type Description Default path str the location and name of the PolyFuzz file you want to load required Usage: PolyFuzz . load ( \"my_model\" )","title":"load()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.match","text":"Show source code in polyfuzz\\polyfuzz.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , top_n : int = 1 ): \"\"\" Match the from_list of strings to the to_list of strings with whatever models you have initialized Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to top_n: The number of matches you want returned. This is currently only implemented for `polyfuzz.models.TFIDF` and `polyfuzz.models.Embeddings` as they can computationally handle more comparisons. Updates: self.matches: A dictionary with the matches from all models, can be accessed with `model.get_all_matches` or `model.get_match(\"TF-IDF\")` Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can access the results matches with `model.get_all_matches` or a specific model with `model.get_match(\"TF-IDF\")` based on their model_id. \"\"\" # Standard models - quick access if isinstance ( self . method , str ): if self . method in [ \"TF-IDF\" , \"TFIDF\" ]: self . method = TFIDF ( min_similarity = 0 , top_n = top_n ) self . matches = { \"TF-IDF\" : self . method . match ( from_list , to_list )} elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: self . method = RapidFuzz () self . matches = { \"EditDistance\" : self . method . match ( from_list , to_list )} elif self . method in [ \"Embeddings\" , \"Embedding\" ]: self . method = Embeddings ( min_similarity = 0 , top_n = top_n ) self . matches = { \"Embeddings\" : self . method . match ( from_list , to_list )} else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" ) logger . info ( f \"Ran model with model id = { self . method } \" ) # Custom models elif isinstance ( self . method , BaseMatcher ): self . matches = { self . method . model_id : self . method . match ( from_list , to_list )} logging . info ( f \"Ran model with model id = { self . method . model_id } \" ) # Multiple custom models elif isinstance ( self . method , Iterable ): self . _update_model_ids () self . matches = {} for model in self . method : self . matches [ model . model_id ] = model . match ( from_list , to_list ) logging . info ( f \"Ran model with model id = { model . model_id } \" ) return self Match the from_list of strings to the to_list of strings with whatever models you have initialized Parameters Name Type Description Default from_list List[str] The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the from_list and ignore the to_list . required to_list List[str] The list where you want to map to None top_n int The number of matches you want returned. This is currently only implemented for polyfuzz.models.TFIDF and polyfuzz.models.Embeddings as they can computationally handle more comparisons. 1 Updates: self.matches: A dictionary with the matches from all models, can be accessed with model.get_all_matches or model.get_match(\"TF-IDF\") Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . match ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) You can access the results matches with model.get_all_matches or a specific model with model.get_match(\"TF-IDF\") based on their model_id.","title":"match()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.save","text":"Show source code in polyfuzz\\polyfuzz.py 429 430 431 432 433 434 435 436 437 438 439 440 441 def save ( self , path : str ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save Usage: ```python model.save(\"my_model\") ``` \"\"\" with open ( path , 'wb' ) as file : joblib . dump ( self , file ) Saves the model to the specified path Parameters Name Type Description Default path str the location and name of the file you want to save required Usage: model . save ( \"my_model\" )","title":"save()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.transform","text":"Show source code in polyfuzz\\polyfuzz.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def transform ( self , from_list : List [ str ]) -> Mapping [ str , pd . DataFrame ]: \"\"\" After fitting your model, match all words in `from_list` to the words that were fitted on previously. Arguments: from_list: The list from which you want mappings. Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit([\"input_string_1\", \"input_string2\"]) ``` Then, you can transform and normalize new strings: ```python results = model.transform([\"input_string_1\", \"input_string2\"]) ``` \"\"\" all_matches = {} if isinstance ( self . method , BaseMatcher ): matches = self . method . match ( from_list , self . to_list , re_train = False ) all_matches [ self . method . type ] = matches elif isinstance ( self . method , Iterable ): for model in self . method : all_matches [ model . type ] = model . match ( from_list , self . to_list , re_train = False ) return all_matches After fitting your model, match all words in from_list to the words that were fitted on previously. Parameters Name Type Description Default from_list List[str] The list from which you want mappings. required Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . fit ([ \"input_string_1\" , \"input_string2\" ]) Then, you can transform and normalize new strings: results = model . transform ([ \"input_string_1\" , \"input_string2\" ])","title":"transform()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.visualize_precision_recall","text":"Show source code in polyfuzz\\polyfuzz.py 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 def visualize_precision_recall ( self , kde : bool = False , save_path : str = None ): \"\"\" Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) model.visualize_precision_recall(save_path=\"results.png\") ``` \"\"\" check_matches ( self ) self . min_precisions = {} self . recalls = {} self . average_precisions = {} for name , match in self . matches . items (): min_precision , recall , average_precision = precision_recall_curve ( match ) self . min_precisions [ name ] = min_precision self . recalls [ name ] = recall self . average_precisions [ name ] = average_precision visualize_precision_recall ( self . matches , self . min_precisions , self . recalls , kde , save_path ) Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters Name Type Description Default kde bool whether to also visualize the kde plot False save_path str the path to save the resulting image to None Usage: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . match ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) model . visualize_precision_recall ( save_path = \"results.png\" )","title":"visualize_precision_recall()"},{"location":"api/models/base/","text":"polyfuzz.models.BaseMatcher \u00b6 The abstract BaseMatching to be modelled after for string matching match ( self , from_list , to_list = None , ** kwargs ) \u00b6 Show source code in models\\_base.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @abstractmethod def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Make sure you follow the same argument structure: Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\" \"\"\" raise NotImplementedError () Make sure you follow the same argument structure: Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None Returns Type Description DataFrame matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\"","title":"BaseMatcher"},{"location":"api/models/base/#polyfuzzmodelsbasematcher","text":"The abstract BaseMatching to be modelled after for string matching","title":"polyfuzz.models.BaseMatcher"},{"location":"api/models/base/#polyfuzz.models._base.BaseMatcher.match","text":"Show source code in models\\_base.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @abstractmethod def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Make sure you follow the same argument structure: Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\" \"\"\" raise NotImplementedError () Make sure you follow the same argument structure: Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None Returns Type Description DataFrame matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\"","title":"match()"},{"location":"api/models/distance/","text":"polyfuzz.models.EditDistance \u00b6 Calculate the Edit Distance between lists of strings using any distance/similarity based scorer Parameters Name Type Description Default n_jobs int Nr of parallel processes, use -1 to use all cores 1 scorer Callable The scorer function to be used to calculate the edit distance. This function should give back a float between 0 and 1, and work as follows: scorer(\"string_one\", \"string_two\") <cyfunction ratio at 0x00000237A334AAD0> model_id str The name of the particular instance, used when comparing models None Usage: from rapidfuzz import fuzz model = EditDistance ( n_jobs =- 1 , scorer = fuzz . WRatio ) match ( self , from_list , to_list = None , ** kwargs ) \u00b6 Show source code in models\\_distance.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if to_list is None : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) to_list = from_list . copy () else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) if self . normalize : matches [ \"Similarity\" ] = ( matches [ \"Similarity\" ] - matches [ \"Similarity\" ] . min ()) / ( matches [ \"Similarity\" ] . max () - matches [ \"Similarity\" ] . min ()) return matches Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = EditDistance ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"EditDistance"},{"location":"api/models/distance/#polyfuzzmodelseditdistance","text":"Calculate the Edit Distance between lists of strings using any distance/similarity based scorer Parameters Name Type Description Default n_jobs int Nr of parallel processes, use -1 to use all cores 1 scorer Callable The scorer function to be used to calculate the edit distance. This function should give back a float between 0 and 1, and work as follows: scorer(\"string_one\", \"string_two\") <cyfunction ratio at 0x00000237A334AAD0> model_id str The name of the particular instance, used when comparing models None Usage: from rapidfuzz import fuzz model = EditDistance ( n_jobs =- 1 , scorer = fuzz . WRatio )","title":"polyfuzz.models.EditDistance"},{"location":"api/models/distance/#polyfuzz.models._distance.EditDistance.match","text":"Show source code in models\\_distance.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if to_list is None : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) to_list = from_list . copy () else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) if self . normalize : matches [ \"Similarity\" ] = ( matches [ \"Similarity\" ] - matches [ \"Similarity\" ] . min ()) / ( matches [ \"Similarity\" ] . max () - matches [ \"Similarity\" ] . min ()) return matches Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = EditDistance ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"match()"},{"location":"api/models/embeddings/","text":"polyfuzz.models.Embeddings \u00b6 Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters Name Type Description Default embedding_method Optional[List] list of Flair embeddings to use None min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: model = Embeddings ( min_similarity = 0.5 ) Or if you want a custom model to be used and it is a word embedding model, pass it in as a list: embedding_model = WordEmbeddings ( 'news' ) model = Embeddings ([ embeddings_model ], min_similarity = 0.5 ) As you might have guessed, you can pass along multiple word embedding models and the results will be averaged: fasttext_embedding = WordEmbeddings ( 'news' ) glove_embedding = WordEmbeddings ( 'glove' ) bert_embedding = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) model = Embeddings ([ glove_embedding , fasttext_embedding , bert_embedding ], min_similarity = 0.5 ) match ( self , from_list , to_list = None , embeddings_from = None , embeddings_to = None , re_train = True ) \u00b6 Show source code in models\\_embeddings.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" # Extract embeddings from the `from_list` if not isinstance ( embeddings_from , np . ndarray ): embeddings_from = self . _embed ( from_list ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . _embed ( from_list ) else : embeddings_to = self . _embed ( to_list ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) self . embeddings_to = embeddings_to return matches Matches the two lists of strings to each other and returns the best mapping Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: model = Embeddings ( min_similarity = 0.5 ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"Embeddings"},{"location":"api/models/embeddings/#polyfuzzmodelsembeddings","text":"Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters Name Type Description Default embedding_method Optional[List] list of Flair embeddings to use None min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: model = Embeddings ( min_similarity = 0.5 ) Or if you want a custom model to be used and it is a word embedding model, pass it in as a list: embedding_model = WordEmbeddings ( 'news' ) model = Embeddings ([ embeddings_model ], min_similarity = 0.5 ) As you might have guessed, you can pass along multiple word embedding models and the results will be averaged: fasttext_embedding = WordEmbeddings ( 'news' ) glove_embedding = WordEmbeddings ( 'glove' ) bert_embedding = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) model = Embeddings ([ glove_embedding , fasttext_embedding , bert_embedding ], min_similarity = 0.5 )","title":"polyfuzz.models.Embeddings"},{"location":"api/models/embeddings/#polyfuzz.models._embeddings.Embeddings.match","text":"Show source code in models\\_embeddings.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" # Extract embeddings from the `from_list` if not isinstance ( embeddings_from , np . ndarray ): embeddings_from = self . _embed ( from_list ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . _embed ( from_list ) else : embeddings_to = self . _embed ( to_list ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) self . embeddings_to = embeddings_to return matches Matches the two lists of strings to each other and returns the best mapping Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: model = Embeddings ( min_similarity = 0.5 ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"match()"},{"location":"api/models/gensim/","text":"polyfuzz.models.GensimEmbeddings \u00b6 Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters Name Type Description Default embedding_model Union[str, gensim.models.keyedvectors.KeyedVectors] The Gensim model to use, this can be either a string or the model directly 'fasttext-wiki-news-subwords-300' min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: distance_model = GensimEmbeddings ( \"fasttext-wiki-news-subwords-300\" , min_similarity = 0.5 ) Or if you want to directly pass a Gensim model: import gensim.downloader as api embedding_model = api . load ( \"fasttext-wiki-news-subwords-300\" ) distance_model = GensimEmbeddings ( embedding_model , min_similarity = 0.5 ) match ( self , from_list , to_list = None , embeddings_from = None , embeddings_to = None , re_train = True ) \u00b6 Show source code in models\\_gensim.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" # Extract embeddings from the `from_list` if not isinstance ( embeddings_from , np . ndarray ): embeddings_from = self . _embed ( from_list ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . _embed ( from_list ) else : embeddings_to = self . _embed ( to_list ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) self . embeddings_to = embeddings_to return matches Matches the two lists of strings to each other and returns the best mapping Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: model = Embeddings ( min_similarity = 0.5 ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"Gensim"},{"location":"api/models/gensim/#polyfuzzmodelsgensimembeddings","text":"Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters Name Type Description Default embedding_model Union[str, gensim.models.keyedvectors.KeyedVectors] The Gensim model to use, this can be either a string or the model directly 'fasttext-wiki-news-subwords-300' min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: distance_model = GensimEmbeddings ( \"fasttext-wiki-news-subwords-300\" , min_similarity = 0.5 ) Or if you want to directly pass a Gensim model: import gensim.downloader as api embedding_model = api . load ( \"fasttext-wiki-news-subwords-300\" ) distance_model = GensimEmbeddings ( embedding_model , min_similarity = 0.5 )","title":"polyfuzz.models.GensimEmbeddings"},{"location":"api/models/gensim/#polyfuzz.models._gensim.GensimEmbeddings.match","text":"Show source code in models\\_gensim.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" # Extract embeddings from the `from_list` if not isinstance ( embeddings_from , np . ndarray ): embeddings_from = self . _embed ( from_list ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . _embed ( from_list ) else : embeddings_to = self . _embed ( to_list ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) self . embeddings_to = embeddings_to return matches Matches the two lists of strings to each other and returns the best mapping Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: model = Embeddings ( min_similarity = 0.5 ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"match()"},{"location":"api/models/matches/","text":"polyfuzz.models.cosine_similarity \u00b6 Calculate similarity between two matrices/vectors and return best matches Parameters Name Type Description Default from_vector ndarray the matrix or vector representing the embedded strings to map from required to_vector ndarray the matrix or vector representing the embedded strings to map to required from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: Make sure to fill the to_vector and from_vector with vector representations of to_list and from_list respectively: from polyfuzz.models import extract_best_matches indices , similarity = extract_best_matches ( from_vector , to_vector , method = \"sparse\" )","title":"CosineSimilarity"},{"location":"api/models/matches/#polyfuzzmodelscosine_similarity","text":"Calculate similarity between two matrices/vectors and return best matches Parameters Name Type Description Default from_vector ndarray the matrix or vector representing the embedded strings to map from required to_vector ndarray the matrix or vector representing the embedded strings to map to required from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: Make sure to fill the to_vector and from_vector with vector representations of to_list and from_list respectively: from polyfuzz.models import extract_best_matches indices , similarity = extract_best_matches ( from_vector , to_vector , method = \"sparse\" )","title":"polyfuzz.models.cosine_similarity"},{"location":"api/models/rapidfuzz/","text":"polyfuzz.models.RapidFuzz \u00b6 Calculate the Edit Distance between lists of strings using RapidFuzz's process function We are using RapidFuzz instead of FuzzyWuzzy since it is much faster and does not require the more restrictive GPL license Parameters Name Type Description Default n_jobs int Nr of parallel processes, use -1 to use all cores 1 score_cutoff float The minimum similarity for which to return a good match. Should be between 0 and 1. 0 scorer Callable The scorer function to be used to calculate the edit distance Options: * fuzz.ratio * fuzz.partial_ratio * fuzz.token_sort_ratio * fuzz.partial_token_sort_ratio * fuzz.token_set_ratio * fuzz.partial_token_set_ratio * fuzz.token_ratio * fuzz.partial_token_ratio * fuzz.WRation * fuzz.QRatio See https://maxbachmann.github.io/rapidfuzz/usage/fuzz/ for an extensive description of the scoring methods. <cyfunction WRatio at 0x00000237A334D2B0> model_id str The name of the particular instance, used when comparing models None Usage: from rapidfuzz import fuzz model = RapidFuzz ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) match ( self , from_list , to_list = None , ** kwargs ) \u00b6 Show source code in models\\_rapidfuzz.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if to_list is None : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) to_list = from_list . copy () else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) return matches Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = RapidFuzz ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"RapidFuzzy"},{"location":"api/models/rapidfuzz/#polyfuzzmodelsrapidfuzz","text":"Calculate the Edit Distance between lists of strings using RapidFuzz's process function We are using RapidFuzz instead of FuzzyWuzzy since it is much faster and does not require the more restrictive GPL license Parameters Name Type Description Default n_jobs int Nr of parallel processes, use -1 to use all cores 1 score_cutoff float The minimum similarity for which to return a good match. Should be between 0 and 1. 0 scorer Callable The scorer function to be used to calculate the edit distance Options: * fuzz.ratio * fuzz.partial_ratio * fuzz.token_sort_ratio * fuzz.partial_token_sort_ratio * fuzz.token_set_ratio * fuzz.partial_token_set_ratio * fuzz.token_ratio * fuzz.partial_token_ratio * fuzz.WRation * fuzz.QRatio See https://maxbachmann.github.io/rapidfuzz/usage/fuzz/ for an extensive description of the scoring methods. <cyfunction WRatio at 0x00000237A334D2B0> model_id str The name of the particular instance, used when comparing models None Usage: from rapidfuzz import fuzz model = RapidFuzz ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio )","title":"polyfuzz.models.RapidFuzz"},{"location":"api/models/rapidfuzz/#polyfuzz.models._rapidfuzz.RapidFuzz.match","text":"Show source code in models\\_rapidfuzz.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if to_list is None : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) to_list = from_list . copy () else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) return matches Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = RapidFuzz ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"match()"},{"location":"api/models/sbert/","text":"polyfuzz.models.SentenceEmbeddings \u00b6 Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters Name Type Description Default embedding_model Union[str, sentence_transformers.SentenceTransformer.SentenceTransformer] The sbert model to use, this can be either a string or the model directly 'all-MiniLM-L6-v2' min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: distance_model = SentenceEmbeddings ( \"all-MiniLM-L6-v2\" , min_similarity = 0.5 ) Or if you want to directly pass a sbert model: from sentence_transformers import SentenceTransformer embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) distance_model = SentenceEmbeddings ( embedding_model , min_similarity = 0.5 ) match ( self , from_list , to_list = None , embeddings_from = None , embeddings_to = None , re_train = True ) \u00b6 Show source code in models\\_sbert.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" # Extract embeddings from the `from_list` embeddings_from = self . embedding_model . encode ( from_list , show_progress_bar = False ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . embedding_model . encode ( from_list , show_progress_bar = False ) else : embeddings_to = self . embedding_model . encode ( to_list , show_progress_bar = False ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) self . embeddings_to = embeddings_to return matches Matches the two lists of strings to each other and returns the best mapping Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: model = Embeddings ( min_similarity = 0.5 ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"SBERT"},{"location":"api/models/sbert/#polyfuzzmodelssentenceembeddings","text":"Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters Name Type Description Default embedding_model Union[str, sentence_transformers.SentenceTransformer.SentenceTransformer] The sbert model to use, this can be either a string or the model directly 'all-MiniLM-L6-v2' min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: distance_model = SentenceEmbeddings ( \"all-MiniLM-L6-v2\" , min_similarity = 0.5 ) Or if you want to directly pass a sbert model: from sentence_transformers import SentenceTransformer embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) distance_model = SentenceEmbeddings ( embedding_model , min_similarity = 0.5 )","title":"polyfuzz.models.SentenceEmbeddings"},{"location":"api/models/sbert/#polyfuzz.models._sbert.SentenceEmbeddings.match","text":"Show source code in models\\_sbert.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" # Extract embeddings from the `from_list` embeddings_from = self . embedding_model . encode ( from_list , show_progress_bar = False ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . embedding_model . encode ( from_list , show_progress_bar = False ) else : embeddings_to = self . embedding_model . encode ( to_list , show_progress_bar = False ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) self . embeddings_to = embeddings_to return matches Matches the two lists of strings to each other and returns the best mapping Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: model = Embeddings ( min_similarity = 0.5 ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"match()"},{"location":"api/models/spacy/","text":"polyfuzz.models.SpacyEmbeddings \u00b6 Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters Name Type Description Default embedding_model The Spacy model to use, this can be either a string or the model directly 'en_core_web_md' min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: distance_model = SpacyEmbeddings ( \"en_core_web_md\" , min_similarity = 0.5 ) Or if you want to directly pass a Spacy model: import spacy embedding_model = spacy . load ( \"en_core_web_md\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) distance_model = SpacyEmbeddings ( embedding_model , min_similarity = 0.5 ) match ( self , from_list , to_list = None , embeddings_from = None , embeddings_to = None , re_train = True ) \u00b6 Show source code in models\\_spacy.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" # Extract embeddings from the `from_list` if not isinstance ( embeddings_from , np . ndarray ): embeddings_from = self . _embed ( from_list ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . _embed ( from_list ) else : embeddings_to = self . _embed ( to_list ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) self . embeddings_to = embeddings_to return matches Matches the two lists of strings to each other and returns the best mapping Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: model = Embeddings ( min_similarity = 0.5 ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"Spacy"},{"location":"api/models/spacy/#polyfuzzmodelsspacyembeddings","text":"Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters Name Type Description Default embedding_model The Spacy model to use, this can be either a string or the model directly 'en_core_web_md' min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: distance_model = SpacyEmbeddings ( \"en_core_web_md\" , min_similarity = 0.5 ) Or if you want to directly pass a Spacy model: import spacy embedding_model = spacy . load ( \"en_core_web_md\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) distance_model = SpacyEmbeddings ( embedding_model , min_similarity = 0.5 )","title":"polyfuzz.models.SpacyEmbeddings"},{"location":"api/models/spacy/#polyfuzz.models._spacy.SpacyEmbeddings.match","text":"Show source code in models\\_spacy.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" # Extract embeddings from the `from_list` if not isinstance ( embeddings_from , np . ndarray ): embeddings_from = self . _embed ( from_list ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . _embed ( from_list ) else : embeddings_to = self . _embed ( to_list ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) self . embeddings_to = embeddings_to return matches Matches the two lists of strings to each other and returns the best mapping Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: model = Embeddings ( min_similarity = 0.5 ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"match()"},{"location":"api/models/tfidf/","text":"polyfuzz.models.TFIDF \u00b6 A character based n-gram TF-IDF to approximate edit distance We turn a string into, typically of length 3, n-grams. For example, using 3-grams of the \"hotel\" we get ['hot', 'ote', 'tel']. These are then used as input for a TfidfVectorizer in order to create a vector for each word. Then, we simply apply cosine similarity through k-NN Parameters Name Type Description Default n_gram_range Tuple[int, int] The n_gram_range on a character-level (3, 3) clean_string bool Whether to clean the string such that only alphanumerical characters are kept True min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: * sparse * sklearn * knn 'sparse' sparse is the fastest and most memory efficient but requires a package that might be difficult to install sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory model_id: The name of the particular instance, used when comparing models Usage: from polymatcher.models import TFIDF model = TFIDF ( n_gram_range = ( 3 , 3 ), clean_string = True , use_knn = False ) match ( self , from_list , to_list = None , re_train = True ) \u00b6 Show source code in models\\_tfidf.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Match two lists of strings to each other and return the most similar strings Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python from polymatcher.models import TFIDF model = TFIDF() matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" tf_idf_from , tf_idf_to = self . _extract_tf_idf ( from_list , to_list , re_train ) matches = cosine_similarity ( tf_idf_from , tf_idf_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) return matches Match two lists of strings to each other and return the most similar strings Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: from polymatcher.models import TFIDF model = TFIDF () matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"TFIDF"},{"location":"api/models/tfidf/#polyfuzzmodelstfidf","text":"A character based n-gram TF-IDF to approximate edit distance We turn a string into, typically of length 3, n-grams. For example, using 3-grams of the \"hotel\" we get ['hot', 'ote', 'tel']. These are then used as input for a TfidfVectorizer in order to create a vector for each word. Then, we simply apply cosine similarity through k-NN Parameters Name Type Description Default n_gram_range Tuple[int, int] The n_gram_range on a character-level (3, 3) clean_string bool Whether to clean the string such that only alphanumerical characters are kept True min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: * sparse * sklearn * knn 'sparse' sparse is the fastest and most memory efficient but requires a package that might be difficult to install sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory model_id: The name of the particular instance, used when comparing models Usage: from polymatcher.models import TFIDF model = TFIDF ( n_gram_range = ( 3 , 3 ), clean_string = True , use_knn = False )","title":"polyfuzz.models.TFIDF"},{"location":"api/models/tfidf/#polyfuzz.models._tfidf.TFIDF.match","text":"Show source code in models\\_tfidf.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Match two lists of strings to each other and return the most similar strings Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python from polymatcher.models import TFIDF model = TFIDF() matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" tf_idf_from , tf_idf_to = self . _extract_tf_idf ( from_list , to_list , re_train ) matches = cosine_similarity ( tf_idf_from , tf_idf_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) return matches Match two lists of strings to each other and return the most similar strings Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: from polymatcher.models import TFIDF model = TFIDF () matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"match()"},{"location":"api/models/use/","text":"polyfuzz.models.USEEmbeddings \u00b6 Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters Name Type Description Default embedding_model The USE model to use, this can be either a string or the model directly 'https://tfhub.dev/google/universal-sentence-encoder/4' min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: distance_model = USEEmbeddings ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" , min_similarity = 0.5 ) Or if you want to directly pass a USE model: import tensorflow_hub embedding_model = tensorflow_hub . load ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) distance_model = UseEmbeddings ( embedding_model , min_similarity = 0.5 ) match ( self , from_list , to_list = None , embeddings_from = None , embeddings_to = None , re_train = True ) \u00b6 Show source code in models\\_use.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" # Extract embeddings from the `from_list` if not isinstance ( embeddings_from , np . ndarray ): embeddings_from = self . _embed ( from_list ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . _embed ( from_list ) else : embeddings_to = self . _embed ( to_list ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) self . embeddings_to = embeddings_to return matches Matches the two lists of strings to each other and returns the best mapping Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: model = Embeddings ( min_similarity = 0.5 ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"USE"},{"location":"api/models/use/#polyfuzzmodelsuseembeddings","text":"Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters Name Type Description Default embedding_model The USE model to use, this can be either a string or the model directly 'https://tfhub.dev/google/universal-sentence-encoder/4' min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: distance_model = USEEmbeddings ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" , min_similarity = 0.5 ) Or if you want to directly pass a USE model: import tensorflow_hub embedding_model = tensorflow_hub . load ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) distance_model = UseEmbeddings ( embedding_model , min_similarity = 0.5 )","title":"polyfuzz.models.USEEmbeddings"},{"location":"api/models/use/#polyfuzz.models._use.USEEmbeddings.match","text":"Show source code in models\\_use.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def match ( self , from_list : List [ str ], to_list : List [ str ] = None , embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" # Extract embeddings from the `from_list` if not isinstance ( embeddings_from , np . ndarray ): embeddings_from = self . _embed ( from_list ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . _embed ( from_list ) else : embeddings_to = self . _embed ( to_list ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) self . embeddings_to = embeddings_to return matches Matches the two lists of strings to each other and returns the best mapping Parameters Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns Type Description DataFrame matches: The best matches between the lists of strings Usage: model = Embeddings ( min_similarity = 0.5 ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ])","title":"match()"},{"location":"tutorial/basematcher/basematcher/","text":"Custom Models \u00b6 Although PolyFuzz has several models implemented, what if you have developed your own? What if you want a different similarity/distance measure that is not defined in PolyFuzz? That is where custom models come in. If you follow the structure of PolyFuzz's BaseMatcher you can quickly implement any model you would like. You simply create a class using BaseMatcher , make sure it has a function match that inputs two lists and outputs a pandas dataframe. That's it! We start by creating our own model that implements the ratio similarity measure from RapidFuzz: import numpy as np import pandas as pd from rapidfuzz import fuzz from polyfuzz import PolyFuzz from polyfuzz.models import BaseMatcher class MyModel ( BaseMatcher ): def match ( self , from_list , to_list , ** kwargs ): # Calculate distances matches = [[ fuzz . ratio ( from_string , to_string ) / 100 for to_string in to_list ] for from_string in from_list ] # Get best matches mappings = [ to_list [ index ] for index in np . argmax ( matches , axis = 1 )] scores = np . max ( matches , axis = 1 ) # Prepare dataframe matches = pd . DataFrame ({ 'From' : from_list , 'To' : mappings , 'Similarity' : scores }) return matches MyModel can now be used within PolyFuzz and runs like every other model: from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] custom_matcher = MyModel () model = PolyFuzz ( custom_matcher ) . match ( from_list , to_list ) Now we can visualize the results: model . visualize_precision_recall ( kde = True ) fit, transform, fit_transform \u00b6 Although the above model can be used in production using fit , it does not track its state between fit and transform . This is not necessary here, since edit distances should be recalculated but if you have embeddings that you do not want to re-calculate, then it is helpful to track the states between fit and transform so that embeddings do not need to be re-calculated. To do so, we can use the re_train parameter to define what happens if we re-train a model (for example when using fit ) and what happens when we do not re-train a model (for example when using transform ). In the example below, when we set re_train=True we calculate the embeddings from both the from_list and to_list if they are defined and save the embeddings to the self.embeddings_to variable. Then, when we set re_train=True , we can prevent redoing the fit by leveraging the pre-calculated self.embeddings_to variable. import numpy as np from sentence_transformers import SentenceTransformer from ._utils import cosine_similarity from ._base import BaseMatcher class SentenceEmbeddings ( BaseMatcher ): def __init__ ( self , model_id ): super () . __init__ ( model_id ) self . type = \"Embeddings\" self . embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) self . embeddings_to = None def match ( self , from_list , to_list , re_train = True ) -> pd . DataFrame : # Extract embeddings from the `from_list` embeddings_from = self . embedding_model . encode ( from_list , show_progress_bar = False ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . embedding_model . encode ( from_list , show_progress_bar = False ) else : embeddings_to = self . embedding_model . encode ( to_list , show_progress_bar = False ) # Extract matches matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list ) self . embeddings_to = embeddings_to return matches Then, we can use it as follows: from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] custom_matcher = MyModel () model = PolyFuzz ( custom_matcher ) . fit ( from_list ) By using the .fit function, embeddings are created from the from_list variable and saved. Then, when we run model.transform(to_list) , the embeddings created from the from_list variable do not need to be recalculated.","title":"Custom Models"},{"location":"tutorial/basematcher/basematcher/#custom-models","text":"Although PolyFuzz has several models implemented, what if you have developed your own? What if you want a different similarity/distance measure that is not defined in PolyFuzz? That is where custom models come in. If you follow the structure of PolyFuzz's BaseMatcher you can quickly implement any model you would like. You simply create a class using BaseMatcher , make sure it has a function match that inputs two lists and outputs a pandas dataframe. That's it! We start by creating our own model that implements the ratio similarity measure from RapidFuzz: import numpy as np import pandas as pd from rapidfuzz import fuzz from polyfuzz import PolyFuzz from polyfuzz.models import BaseMatcher class MyModel ( BaseMatcher ): def match ( self , from_list , to_list , ** kwargs ): # Calculate distances matches = [[ fuzz . ratio ( from_string , to_string ) / 100 for to_string in to_list ] for from_string in from_list ] # Get best matches mappings = [ to_list [ index ] for index in np . argmax ( matches , axis = 1 )] scores = np . max ( matches , axis = 1 ) # Prepare dataframe matches = pd . DataFrame ({ 'From' : from_list , 'To' : mappings , 'Similarity' : scores }) return matches MyModel can now be used within PolyFuzz and runs like every other model: from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] custom_matcher = MyModel () model = PolyFuzz ( custom_matcher ) . match ( from_list , to_list ) Now we can visualize the results: model . visualize_precision_recall ( kde = True )","title":"Custom Models"},{"location":"tutorial/basematcher/basematcher/#fit-transform-fit_transform","text":"Although the above model can be used in production using fit , it does not track its state between fit and transform . This is not necessary here, since edit distances should be recalculated but if you have embeddings that you do not want to re-calculate, then it is helpful to track the states between fit and transform so that embeddings do not need to be re-calculated. To do so, we can use the re_train parameter to define what happens if we re-train a model (for example when using fit ) and what happens when we do not re-train a model (for example when using transform ). In the example below, when we set re_train=True we calculate the embeddings from both the from_list and to_list if they are defined and save the embeddings to the self.embeddings_to variable. Then, when we set re_train=True , we can prevent redoing the fit by leveraging the pre-calculated self.embeddings_to variable. import numpy as np from sentence_transformers import SentenceTransformer from ._utils import cosine_similarity from ._base import BaseMatcher class SentenceEmbeddings ( BaseMatcher ): def __init__ ( self , model_id ): super () . __init__ ( model_id ) self . type = \"Embeddings\" self . embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) self . embeddings_to = None def match ( self , from_list , to_list , re_train = True ) -> pd . DataFrame : # Extract embeddings from the `from_list` embeddings_from = self . embedding_model . encode ( from_list , show_progress_bar = False ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . embedding_model . encode ( from_list , show_progress_bar = False ) else : embeddings_to = self . embedding_model . encode ( to_list , show_progress_bar = False ) # Extract matches matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list ) self . embeddings_to = embeddings_to return matches Then, we can use it as follows: from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] custom_matcher = MyModel () model = PolyFuzz ( custom_matcher ) . fit ( from_list ) By using the .fit function, embeddings are created from the from_list variable and saved. Then, when we run model.transform(to_list) , the embeddings created from the from_list variable do not need to be recalculated.","title":"fit, transform, fit_transform"},{"location":"tutorial/datasets/datasets/","text":"Datasets \u00b6 There are two datasets prepared for you to play around with: Company Names Movie Titles Movie Titles \u00b6 This data is retrieved from: https://www.kaggle.com/stefanoleone992/imdb-extensive-dataset https://www.kaggle.com/shivamb/netflix-shows It contains Netflix and IMDB movie titles that can be matched against each other. Where IMDB has 80852 movie titles and Netflix has 6172 movie titles. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_movie_titles data = load_movie_titles () model = PolyFuzz ( \"TF-IDF\" ) . match ( data [ \"Netflix\" ], data [ \"IMDB\" ]) Company Names \u00b6 This data is retrieved from here and contains 100_000 company names to be matched against each other. This is a different use case than what you have typically seen so far. We often see two different lists compared with each other. Here, you can use this dataset to compare the company names with themselves in order to clean them up. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_company_names data = load_company_names () model = PolyFuzz ( \"TF-IDF\" ) . match ( data ) By only inserting a single list, PolyFuzz will recognize that you are looking to match the titles with themselves. It will ignore any comparison a string has with itself, otherwise everything will get mapped to itself.","title":"Datasets"},{"location":"tutorial/datasets/datasets/#datasets","text":"There are two datasets prepared for you to play around with: Company Names Movie Titles","title":"Datasets"},{"location":"tutorial/datasets/datasets/#movie-titles","text":"This data is retrieved from: https://www.kaggle.com/stefanoleone992/imdb-extensive-dataset https://www.kaggle.com/shivamb/netflix-shows It contains Netflix and IMDB movie titles that can be matched against each other. Where IMDB has 80852 movie titles and Netflix has 6172 movie titles. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_movie_titles data = load_movie_titles () model = PolyFuzz ( \"TF-IDF\" ) . match ( data [ \"Netflix\" ], data [ \"IMDB\" ])","title":"Movie Titles"},{"location":"tutorial/datasets/datasets/#company-names","text":"This data is retrieved from here and contains 100_000 company names to be matched against each other. This is a different use case than what you have typically seen so far. We often see two different lists compared with each other. Here, you can use this dataset to compare the company names with themselves in order to clean them up. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_company_names data = load_company_names () model = PolyFuzz ( \"TF-IDF\" ) . match ( data ) By only inserting a single list, PolyFuzz will recognize that you are looking to match the titles with themselves. It will ignore any comparison a string has with itself, otherwise everything will get mapped to itself.","title":"Company Names"},{"location":"tutorial/grouper/grouper/","text":"Custom Grouper \u00b6 The basic grouper is a TF-IDF implementation that uses single linkage to group the strings you mapped to together. With the customizability philosophy of PolyFuzz in mind it is not unexpected that you can also use any of the models, and even custom models, as your grouper! Here, we use Edit Distance instead of TF-IDF to group the strings we mapped to: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] model = PolyFuzz ( \"TF-IDF\" ) . match ( from_list , to_list ) # Custom grouper base_edit_grouper = EditDistance ( n_jobs = 1 ) model . group ( base_edit_grouper ) And that is it! We have now grouped our matches we mapped to together using Edit Distance instead of TF-IDF.","title":"Custom Grouper"},{"location":"tutorial/grouper/grouper/#custom-grouper","text":"The basic grouper is a TF-IDF implementation that uses single linkage to group the strings you mapped to together. With the customizability philosophy of PolyFuzz in mind it is not unexpected that you can also use any of the models, and even custom models, as your grouper! Here, we use Edit Distance instead of TF-IDF to group the strings we mapped to: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] model = PolyFuzz ( \"TF-IDF\" ) . match ( from_list , to_list ) # Custom grouper base_edit_grouper = EditDistance ( n_jobs = 1 ) model . group ( base_edit_grouper ) And that is it! We have now grouped our matches we mapped to together using Edit Distance instead of TF-IDF.","title":"Custom Grouper"},{"location":"tutorial/models/models/","text":"Models \u00b6 Currently, the following models are implemented in PolyFuzz: 1. TF-IDF 2. EditDistance with RapidFuzz 3. FastText and GloVe 4. \ud83e\udd17 Transformers 5. SentenceTransformers 6. Gensim 7. Spacy With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzz. All models listed above can be found in polyfuzz.models and can be used to create and compare different matchers. TF-IDF \u00b6 Although the terms in TF-IDF are usually words, we are going to be using TF-IDF on a character-level. We will be extracting n-grams from a string and count the frequency of these n-grams across all input strings. For example, with 3-grams, the word \"hotel\" can be defined as \"hot\", \"ote\", and \"tel\". After generating the n-grams and applying TF-IDF on these \"terms\", we can use cosine similarity to compare the generated TF-IDF vectors against each other. We simply load in TFIDF from polyfuzz.models and pass it to our PolyFuzz instance: from polyfuzz.models import TFIDF from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , matcher_id = \"TF-IDF\" ) model = PolyFuzz ( tfidf ) . match ( from_list , to_list ) and that's it! You can play around with the TFIDF matcher until you get the results you are looking for. Note that if you increase the min_similarity there is a chance that some strings will not be matched at all. EditDistance \u00b6 There are many edit distance functions one could use and the EditDistance model from polyfuzz.models allows you to pass in any distance function. As long as that distance function takes in two strings and spits out a float, you can pass anything! In the example below, we are going to be using Jaro Winkler Similarity from the jellyfish package to create our custom scorer: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from jellyfish import jaro_winkler_similarity from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] jellyfish_matcher = EditDistance ( n_jobs = 1 , scorer = jaro_winkler_similarity ) model = PolyFuzz ( jellyfish_matcher ) . match ( from_list , to_list ) RapidFuzz \u00b6 Edit distance measures are typically quite slow. Moreover, the one that is heavily used, fuzzywuzzy , has a very restrictive licence (GPL). Instead, I decided to create a RapidFuzz matcher which is a fast implementation of fuzzywuzzy and has a less restrictive licence (MIT): from polyfuzz import PolyFuzz from polyfuzz.models import RapidFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] rapidfuzz_matcher = RapidFuzz ( n_jobs = 1 ) model = PolyFuzz ( rapidfuzz_matcher ) . match ( from_list , to_list ) Embeddings \u00b6 With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . The embeddings that are created are compared with cosine similarity in order to understand how similar the created embeddings are to each other. We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 ) models = PolyFuzz ( bert_matcher ) . match ( from_list , to_list ) Flair allows you to use pool word embeddings to create more powerful word embeddings. Below, we pool FastText and BERT to create a single embedding representation from which we can calculate the similarity between strings: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings , WordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 ) fasttext = WordEmbeddings ( 'en-crawl' ) fasttext_matcher = Embeddings ( fasttext , min_similarity = 0 ) matchers = [ bert_matcher , fasttext_matcher ] models = PolyFuzz ( matchers ) . match ( from_list , to_list ) SentenceTransformers \u00b6 We can use sentence-transformers to generate embeddings from our input list and find the closest matching entities using cosine similarity. We simply have to instantiate our SentenceEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import SentenceEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = SentenceEmbeddings ( \"all-MiniLM-L6-v2\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom SentenceTransformer model: from polyfuzz import PolyFuzz from polyfuzz.models import SentenceEmbeddings from sentence_transformers import SentenceTransformer from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) distance_model = SentenceEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) Gensim \u00b6 We can use gensim to load in a word embedding model to generate embeddings from our input list and find the closest matching entities using cosine similarity. We simply have to instantiate our GensimEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import GensimEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = GensimEmbeddings ( \"glove-twitter-25\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom Gensim model: from polyfuzz import PolyFuzz from polyfuzz.models import GensimEmbeddings import gensim.downloader as api from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = api . load ( \"glove-twitter-25\" ) distance_model = GensimEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) Spacy \u00b6 We can use spacy to load in an embedding model to generate embeddings from our input list and find the closest matching entities using cosine similarity. We simply have to instantiate our SpacyEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import SpacyEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = SpacyEmbeddings ( \"en_core_web_md\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom Spacy model: from polyfuzz import PolyFuzz from polyfuzz.models import SpacyEmbeddings import spacy from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = spacy . load ( \"en_core_web_md\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) distance_model = SpacyEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) Universal Sentence Encoder (USE) \u00b6 The Universal Sentence Encoder encodes text into high-dimensional vectors that are used here for embedding the strings. The model is trained and optimized for greater-than-word length text, such as sentences, phrases, or short paragraphs. We simply have to instantiate our USEEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import USEEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = USEEmbeddings ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom USE model: from polyfuzz import PolyFuzz from polyfuzz.models import USEEmbeddings import tensorflow_hub from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = tensorflow_hub . load ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) distance_model = USEEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) Using Multiple Models \u00b6 from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance , TFIDF , Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 , matcher_id = \"BERT\" ) tfidf_matcher = TFIDF ( min_similarity = 0 ) edit_matcher = EditDistance () matchers = [ bert_matcher , tfidf_matcher , edit_matcher ] models = PolyFuzz ( matchers ) . match ( from_list , to_list ) To access the results, we again can call get_matches but since we have multiple models we get back a dictionary of dataframes back. In order to access the results of a specific model, call get_matches with the correct id: >>> models . get_matches ( \"BERT\" ) From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.928045 3 recal apples 0.825268 4 house mouse 0.887524 5 similarity mouse 0.791548 Finally, visualize the results to compare the models: models . visualize_precision_recall ( kde = True )","title":"Models"},{"location":"tutorial/models/models/#models","text":"Currently, the following models are implemented in PolyFuzz: 1. TF-IDF 2. EditDistance with RapidFuzz 3. FastText and GloVe 4. \ud83e\udd17 Transformers 5. SentenceTransformers 6. Gensim 7. Spacy With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzz. All models listed above can be found in polyfuzz.models and can be used to create and compare different matchers.","title":"Models"},{"location":"tutorial/models/models/#tf-idf","text":"Although the terms in TF-IDF are usually words, we are going to be using TF-IDF on a character-level. We will be extracting n-grams from a string and count the frequency of these n-grams across all input strings. For example, with 3-grams, the word \"hotel\" can be defined as \"hot\", \"ote\", and \"tel\". After generating the n-grams and applying TF-IDF on these \"terms\", we can use cosine similarity to compare the generated TF-IDF vectors against each other. We simply load in TFIDF from polyfuzz.models and pass it to our PolyFuzz instance: from polyfuzz.models import TFIDF from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , matcher_id = \"TF-IDF\" ) model = PolyFuzz ( tfidf ) . match ( from_list , to_list ) and that's it! You can play around with the TFIDF matcher until you get the results you are looking for. Note that if you increase the min_similarity there is a chance that some strings will not be matched at all.","title":"TF-IDF"},{"location":"tutorial/models/models/#editdistance","text":"There are many edit distance functions one could use and the EditDistance model from polyfuzz.models allows you to pass in any distance function. As long as that distance function takes in two strings and spits out a float, you can pass anything! In the example below, we are going to be using Jaro Winkler Similarity from the jellyfish package to create our custom scorer: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from jellyfish import jaro_winkler_similarity from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] jellyfish_matcher = EditDistance ( n_jobs = 1 , scorer = jaro_winkler_similarity ) model = PolyFuzz ( jellyfish_matcher ) . match ( from_list , to_list )","title":"EditDistance"},{"location":"tutorial/models/models/#rapidfuzz","text":"Edit distance measures are typically quite slow. Moreover, the one that is heavily used, fuzzywuzzy , has a very restrictive licence (GPL). Instead, I decided to create a RapidFuzz matcher which is a fast implementation of fuzzywuzzy and has a less restrictive licence (MIT): from polyfuzz import PolyFuzz from polyfuzz.models import RapidFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] rapidfuzz_matcher = RapidFuzz ( n_jobs = 1 ) model = PolyFuzz ( rapidfuzz_matcher ) . match ( from_list , to_list )","title":"RapidFuzz"},{"location":"tutorial/models/models/#embeddings","text":"With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . The embeddings that are created are compared with cosine similarity in order to understand how similar the created embeddings are to each other. We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 ) models = PolyFuzz ( bert_matcher ) . match ( from_list , to_list ) Flair allows you to use pool word embeddings to create more powerful word embeddings. Below, we pool FastText and BERT to create a single embedding representation from which we can calculate the similarity between strings: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings , WordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 ) fasttext = WordEmbeddings ( 'en-crawl' ) fasttext_matcher = Embeddings ( fasttext , min_similarity = 0 ) matchers = [ bert_matcher , fasttext_matcher ] models = PolyFuzz ( matchers ) . match ( from_list , to_list )","title":"Embeddings"},{"location":"tutorial/models/models/#sentencetransformers","text":"We can use sentence-transformers to generate embeddings from our input list and find the closest matching entities using cosine similarity. We simply have to instantiate our SentenceEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import SentenceEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = SentenceEmbeddings ( \"all-MiniLM-L6-v2\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom SentenceTransformer model: from polyfuzz import PolyFuzz from polyfuzz.models import SentenceEmbeddings from sentence_transformers import SentenceTransformer from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) distance_model = SentenceEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list )","title":"SentenceTransformers"},{"location":"tutorial/models/models/#gensim","text":"We can use gensim to load in a word embedding model to generate embeddings from our input list and find the closest matching entities using cosine similarity. We simply have to instantiate our GensimEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import GensimEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = GensimEmbeddings ( \"glove-twitter-25\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom Gensim model: from polyfuzz import PolyFuzz from polyfuzz.models import GensimEmbeddings import gensim.downloader as api from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = api . load ( \"glove-twitter-25\" ) distance_model = GensimEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list )","title":"Gensim"},{"location":"tutorial/models/models/#spacy","text":"We can use spacy to load in an embedding model to generate embeddings from our input list and find the closest matching entities using cosine similarity. We simply have to instantiate our SpacyEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import SpacyEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = SpacyEmbeddings ( \"en_core_web_md\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom Spacy model: from polyfuzz import PolyFuzz from polyfuzz.models import SpacyEmbeddings import spacy from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = spacy . load ( \"en_core_web_md\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) distance_model = SpacyEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list )","title":"Spacy"},{"location":"tutorial/models/models/#universal-sentence-encoder-use","text":"The Universal Sentence Encoder encodes text into high-dimensional vectors that are used here for embedding the strings. The model is trained and optimized for greater-than-word length text, such as sentences, phrases, or short paragraphs. We simply have to instantiate our USEEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import USEEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = USEEmbeddings ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom USE model: from polyfuzz import PolyFuzz from polyfuzz.models import USEEmbeddings import tensorflow_hub from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = tensorflow_hub . load ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) distance_model = USEEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list )","title":"Universal Sentence Encoder (USE)"},{"location":"tutorial/models/models/#using-multiple-models","text":"from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance , TFIDF , Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 , matcher_id = \"BERT\" ) tfidf_matcher = TFIDF ( min_similarity = 0 ) edit_matcher = EditDistance () matchers = [ bert_matcher , tfidf_matcher , edit_matcher ] models = PolyFuzz ( matchers ) . match ( from_list , to_list ) To access the results, we again can call get_matches but since we have multiple models we get back a dictionary of dataframes back. In order to access the results of a specific model, call get_matches with the correct id: >>> models . get_matches ( \"BERT\" ) From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.928045 3 recal apples 0.825268 4 house mouse 0.887524 5 similarity mouse 0.791548 Finally, visualize the results to compare the models: models . visualize_precision_recall ( kde = True )","title":"Using Multiple Models"},{"location":"tutorial/quickstart/quickstart/","text":"Installation \u00b6 You can install PolyFuzz via pip: pip install polyfuzz You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: pip install bertopic [ sbert ] pip install bertopic [ flair ] pip install bertopic [ gensim ] pip install bertopic [ spacy ] pip install bertopic [ use ] If you want to speed up the cosine similarity comparison and decrease memory usage when using embedding models, you can use sparse_dot_topn which is installed via: pip install polyfuzz [ fast ] Getting Started \u00b6 The main goal of PolyFuzz is to allow the user to perform different methods for matching strings. We start by defining two lists, one to map from and one to map to. We are going to be using TF-IDF to create n-grams on a character level in order to compare similarity between strings. We only have to instantiate PolyFuzz with TF-IDF and match the lists: from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] model = PolyFuzz ( \"TF-IDF\" ) . match ( from_list , to_list ) The resulting matches can be accessed through model.get_matches() : >>> model . get_matches () From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.783751 3 recal None 0.000000 4 house mouse 0.587927 5 similarity None 0.000000 NOTE : When instantiating PolyFuzz we also could have used \"EditDistance\" or \"Embeddings\" to quickly access Levenshtein and FastText (English) respectively. Fit / Transform \u00b6 The .match function allows you to quickly extract similar strings. However, after selecting the right models to be used, you may want to use PolyFuzz in production to match incoming strings. To do so, we can make use of the familiar fit , transform , and fit_transform functions. Let's say that we have a list of words that we know to be correct called train_words . We want to any incoming word to mapped to one of the words in train_words . In other words, we fit on train_words and we use transform on any incoming words: from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer from polyfuzz import PolyFuzz train_words = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] unseen_words = [ \"apple\" , \"apples\" , \"mouse\" ] # Fit model = PolyFuzz ( \"TF-IDF\" ) model . fit ( train_words ) # Transform results = model . transform ( unseen_words ) In the above example, we are using fit on train_words to calculate the TF-IDF representation of those words which are saved to be used again in transform . This speeds up transform quite a bit since all TF-IDF representations are stored when applying fit . Save / Load \u00b6 We can save and load the model as follows to be used in production: # Save the model model . save ( \"my_model\" ) # Load the model loaded_model = PolyFuzz . load ( \"my_model\" ) Group Matches \u00b6 We can group the matches To as there might be significant overlap in strings in our to_list. To do this, we calculate the similarity within strings in to_list and use single linkage to then group the strings with a high similarity. When we extract the new matches, we can see an additional column Group in which all the To matches were grouped to: >>> model . group ( link_min_similarity = 0.75 ) >>> model . get_matches () From To Similarity Group 0 apple apple 1.000000 apples 1 apples apples 1.000000 apples 2 appl apple 0.783751 apples 3 recal None 0.000000 None 4 house mouse 0.587927 mouse 5 similarity None 0.000000 None As can be seen above, we grouped apple and apples together to apple such that when a string is mapped to apple it will fall in the cluster of [apples, apple] and will be mapped to the first instance in the cluster which is apples . Precision-Recall Curve \u00b6 Next, we would like to see how well our model is doing on our data. We express our results as precision and recall where precision is defined as the minimum similarity score before a match is correct and recall the percentage of matches found at a certain minimum similarity score. Creating the visualizations is as simple as: model.visualize_precision_recall()","title":"Quickstart"},{"location":"tutorial/quickstart/quickstart/#installation","text":"You can install PolyFuzz via pip: pip install polyfuzz You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: pip install bertopic [ sbert ] pip install bertopic [ flair ] pip install bertopic [ gensim ] pip install bertopic [ spacy ] pip install bertopic [ use ] If you want to speed up the cosine similarity comparison and decrease memory usage when using embedding models, you can use sparse_dot_topn which is installed via: pip install polyfuzz [ fast ]","title":"Installation"},{"location":"tutorial/quickstart/quickstart/#getting-started","text":"The main goal of PolyFuzz is to allow the user to perform different methods for matching strings. We start by defining two lists, one to map from and one to map to. We are going to be using TF-IDF to create n-grams on a character level in order to compare similarity between strings. We only have to instantiate PolyFuzz with TF-IDF and match the lists: from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] model = PolyFuzz ( \"TF-IDF\" ) . match ( from_list , to_list ) The resulting matches can be accessed through model.get_matches() : >>> model . get_matches () From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.783751 3 recal None 0.000000 4 house mouse 0.587927 5 similarity None 0.000000 NOTE : When instantiating PolyFuzz we also could have used \"EditDistance\" or \"Embeddings\" to quickly access Levenshtein and FastText (English) respectively.","title":"Getting Started"},{"location":"tutorial/quickstart/quickstart/#fit-transform","text":"The .match function allows you to quickly extract similar strings. However, after selecting the right models to be used, you may want to use PolyFuzz in production to match incoming strings. To do so, we can make use of the familiar fit , transform , and fit_transform functions. Let's say that we have a list of words that we know to be correct called train_words . We want to any incoming word to mapped to one of the words in train_words . In other words, we fit on train_words and we use transform on any incoming words: from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer from polyfuzz import PolyFuzz train_words = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] unseen_words = [ \"apple\" , \"apples\" , \"mouse\" ] # Fit model = PolyFuzz ( \"TF-IDF\" ) model . fit ( train_words ) # Transform results = model . transform ( unseen_words ) In the above example, we are using fit on train_words to calculate the TF-IDF representation of those words which are saved to be used again in transform . This speeds up transform quite a bit since all TF-IDF representations are stored when applying fit .","title":"Fit / Transform"},{"location":"tutorial/quickstart/quickstart/#save-load","text":"We can save and load the model as follows to be used in production: # Save the model model . save ( \"my_model\" ) # Load the model loaded_model = PolyFuzz . load ( \"my_model\" )","title":"Save / Load"},{"location":"tutorial/quickstart/quickstart/#group-matches","text":"We can group the matches To as there might be significant overlap in strings in our to_list. To do this, we calculate the similarity within strings in to_list and use single linkage to then group the strings with a high similarity. When we extract the new matches, we can see an additional column Group in which all the To matches were grouped to: >>> model . group ( link_min_similarity = 0.75 ) >>> model . get_matches () From To Similarity Group 0 apple apple 1.000000 apples 1 apples apples 1.000000 apples 2 appl apple 0.783751 apples 3 recal None 0.000000 None 4 house mouse 0.587927 mouse 5 similarity None 0.000000 None As can be seen above, we grouped apple and apples together to apple such that when a string is mapped to apple it will fall in the cluster of [apples, apple] and will be mapped to the first instance in the cluster which is apples .","title":"Group Matches"},{"location":"tutorial/quickstart/quickstart/#precision-recall-curve","text":"Next, we would like to see how well our model is doing on our data. We express our results as precision and recall where precision is defined as the minimum similarity score before a match is correct and recall the percentage of matches found at a certain minimum similarity score. Creating the visualizations is as simple as: model.visualize_precision_recall()","title":"Precision-Recall Curve"}]}