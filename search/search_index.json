{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PolyFuzz \u00b6 PolyFuzz performs fuzzy string matching, string grouping, and contains extensive evaluation functions. PolyFuzz is meant to bring fuzzy string matching techniques together within a single framework. Currently, methods include Levenshtein distance with RapidFuzz, a character-based n-gram TF-IDF, word embedding techniques such as FastText and GloVe, and \ud83e\udd17 transformers embeddings. The philosophy of PolyFuzz is: Easy to use yet highly customizable . It is a string matcher tool that requires only a few lines of code but that allows you customize and create your own models. Installation \u00b6 You can install PolyFuzz via pip: pip install polyfuzz This will install the base dependencies and excludes any deep learning/embedding models. If you want to be making use of \ud83e\udd17 Transformers, install the additional additional Flair dependency: pip install polyfuzz[flair]","title":"Home"},{"location":"#polyfuzz","text":"PolyFuzz performs fuzzy string matching, string grouping, and contains extensive evaluation functions. PolyFuzz is meant to bring fuzzy string matching techniques together within a single framework. Currently, methods include Levenshtein distance with RapidFuzz, a character-based n-gram TF-IDF, word embedding techniques such as FastText and GloVe, and \ud83e\udd17 transformers embeddings. The philosophy of PolyFuzz is: Easy to use yet highly customizable . It is a string matcher tool that requires only a few lines of code but that allows you customize and create your own models.","title":"PolyFuzz"},{"location":"#installation","text":"You can install PolyFuzz via pip: pip install polyfuzz This will install the base dependencies and excludes any deep learning/embedding models. If you want to be making use of \ud83e\udd17 Transformers, install the additional additional Flair dependency: pip install polyfuzz[flair]","title":"Installation"},{"location":"releases/","text":"v0.3.3 - Update numpy to \"numpy>=1.20.0\" to prevent this and this issue - Update pytorch to \"torch>=1.4.0,<1.7.1\" to prevent save_state_warning error v0.3.2 - Fix exploding memory usage when using top_n v0.3.0 - Use top_n in polyfuzz.models.TFIDF and polyfuzz.models.Embeddings v0.2.2 - Update grouping to include all strings only if identical lists of strings are compared v0.2.0 - Update naming convention matcher --> model - Update documentation - Add basic models to grouper - Fix issues with vector order in cosine similarity - Update naming of cosine similarity function v0.1.0 - Additional tests - More thorough documentation - Prepare for public release v0.0.1 - First release of PolyFuzz - Matching through: - Edit Distance - TF-IDF - Embeddings - Custom models - Grouping of results with custom models - Evaluation through precision-recall curves","title":"Releases"},{"location":"api/linkage/","text":"polyfuzz.linkage \u00b6 \u00b6 single_linkage ( matches , min_similarity = 0.8 ) \u00b6 Single linkage clustering from column 'From' to column 'To' matches contains three columns: From , To , and Similarity where Similarity is already the minimum similarity score and thus no checking for minimum similarity is necessary. Parameters: Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for creating groups required min_similarity float minimum similarity between strings before they can be merged into a group 0.8 Returns: Type Description Tuple[Mapping[int, List[str]], Mapping[str, int], Mapping[str, str]] clusters: The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster Source code in polyfuzz\\linkage.py def single_linkage ( matches : pd . DataFrame , min_similarity : float = 0.8 ) -> Tuple [ Mapping [ int , List [ str ]], Mapping [ str , int ], Mapping [ str , str ]]: \"\"\" Single linkage clustering from column 'From' to column 'To' `matches` contains three columns: *From*, *To*, and *Similarity* where *Similarity* is already the minimum similarity score and thus no checking for minimum similarity is necessary. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for creating groups min_similarity: minimum similarity between strings before they can be merged into a group Returns: clusters: The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster \"\"\" matches = matches . loc [ matches . Similarity > min_similarity , :] cluster_mapping = {} cluster_id = 0 for row in matches . itertuples (): # If from string has not already been mapped if not cluster_mapping . get ( row . From ): # If the to string has not already been mapped if not cluster_mapping . get ( row . To ): cluster_mapping [ row . To ] = cluster_id cluster_mapping [ row . From ] = cluster_id cluster_id += 1 # If the to string has already been mapped else : cluster_mapping [ row . From ] = cluster_mapping . get ( row . To ) # Populate the clusters clusters = {} for key , value in cluster_mapping . items (): clusters . setdefault ( value , []) clusters [ value ] . append ( key ) cluster_name_map = { key : clusters . get ( value )[ 0 ] for key , value in cluster_mapping . items ()} return clusters , cluster_mapping , cluster_name_map","title":"Linkage"},{"location":"api/linkage/#polyfuzzlinkage","text":"","title":"polyfuzz.linkage"},{"location":"api/linkage/#polyfuzz.linkage","text":"","title":"polyfuzz.linkage"},{"location":"api/linkage/#polyfuzz.linkage.single_linkage","text":"Single linkage clustering from column 'From' to column 'To' matches contains three columns: From , To , and Similarity where Similarity is already the minimum similarity score and thus no checking for minimum similarity is necessary. Parameters: Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for creating groups required min_similarity float minimum similarity between strings before they can be merged into a group 0.8 Returns: Type Description Tuple[Mapping[int, List[str]], Mapping[str, int], Mapping[str, str]] clusters: The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster Source code in polyfuzz\\linkage.py def single_linkage ( matches : pd . DataFrame , min_similarity : float = 0.8 ) -> Tuple [ Mapping [ int , List [ str ]], Mapping [ str , int ], Mapping [ str , str ]]: \"\"\" Single linkage clustering from column 'From' to column 'To' `matches` contains three columns: *From*, *To*, and *Similarity* where *Similarity* is already the minimum similarity score and thus no checking for minimum similarity is necessary. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for creating groups min_similarity: minimum similarity between strings before they can be merged into a group Returns: clusters: The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster \"\"\" matches = matches . loc [ matches . Similarity > min_similarity , :] cluster_mapping = {} cluster_id = 0 for row in matches . itertuples (): # If from string has not already been mapped if not cluster_mapping . get ( row . From ): # If the to string has not already been mapped if not cluster_mapping . get ( row . To ): cluster_mapping [ row . To ] = cluster_id cluster_mapping [ row . From ] = cluster_id cluster_id += 1 # If the to string has already been mapped else : cluster_mapping [ row . From ] = cluster_mapping . get ( row . To ) # Populate the clusters clusters = {} for key , value in cluster_mapping . items (): clusters . setdefault ( value , []) clusters [ value ] . append ( key ) cluster_name_map = { key : clusters . get ( value )[ 0 ] for key , value in cluster_mapping . items ()} return clusters , cluster_mapping , cluster_name_map","title":"single_linkage()"},{"location":"api/metrics/","text":"polyfuzz.metrics \u00b6 \u00b6 precision_recall_curve ( matches , precision_steps = 0.01 ) \u00b6 Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters: Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for calculating precision, recall, and average precision required precision_steps float the incremental steps in minimum precision 0.01 Returns: Type Description Tuple[List[float], List[float], List[float]] min_precisions: minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step Source code in polyfuzz\\metrics.py def precision_recall_curve ( matches : pd . DataFrame , precision_steps : float = 0.01 ) -> Tuple [ List [ float ], List [ float ], List [ float ]]: \"\"\" Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision precision_steps: the incremental steps in minimum precision Returns: min_precisions: minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step \"\"\" min_precisions = list ( np . arange ( 0. , 1 + precision_steps , precision_steps )) average_precision = [] recall = [] similarities = matches . Similarity . values total = len ( matches ) for min_precision in min_precisions : selection = similarities [ similarities >= min_precision ] recall . append ( len ( selection ) / total ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = RuntimeWarning ) average_precision . append ( float ( np . mean ( selection ))) return min_precisions , recall , average_precision visualize_precision_recall ( matches , min_precisions , recall , kde = True , save_path = None ) \u00b6 Visualize the precision recall curve for one or more models Parameters: Name Type Description Default matches Mapping[str, pandas.core.frame.DataFrame] contains the columns From , To , and Similarity used for calculating precision, recall, and average precision per model required min_precisions Mapping[str, List[float]] minimum precision steps per model required recall Mapping[str, List[float]] recall per minimum precision step per model required kde bool whether to also visualize the kde plot True save_path str the path to save the resulting image to None Usage: visualize_precision_recall(matches, min_precisions, recall, save_path=\"data/results.png\") Source code in polyfuzz\\metrics.py def visualize_precision_recall ( matches : Mapping [ str , pd . DataFrame ], min_precisions : Mapping [ str , List [ float ]], recall : Mapping [ str , List [ float ]], kde : bool = True , save_path : str = None ): \"\"\" Visualize the precision recall curve for one or more models Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision per model min_precisions: minimum precision steps per model recall: recall per minimum precision step per model kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python visualize_precision_recall(matches, min_precisions, recall, save_path=\"data/results.png\") ``` \"\"\" SMALL_SIZE = 10 MEDIUM_SIZE = 12 BIGGER_SIZE = 14 plt . rc ( 'font' , size = SMALL_SIZE ) # controls default text sizes plt . rc ( 'axes' , titlesize = SMALL_SIZE ) # fontsize of the axes title plt . rc ( 'axes' , labelsize = MEDIUM_SIZE ) # fontsize of the x and y labels plt . rc ( 'xtick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'ytick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'legend' , fontsize = SMALL_SIZE ) # legend fontsize plt . rc ( 'figure' , titlesize = BIGGER_SIZE ) # fontsize of the figure title if not isinstance ( matches , dict ): matches = { \"Model\" : matches } min_precisions = { \"Model\" : min_precisions } recall = { \"Model\" : recall } # Create single dataset of similarity score for all models distribution_data = [( matches [ name ] . Similarity . values , [ name for _ in range ( len ( matches [ name ]))]) for name in matches . keys ()] distribution_data = pd . DataFrame ( np . hstack ( distribution_data ) . T , columns = [ \"Similarity\" , \"Model\" ]) distribution_data . Similarity = distribution_data . Similarity . astype ( float ) model_names = list ( matches . keys ()) # Create layout cmap = get_cmap ( 'Accent' ) fig = plt . figure ( figsize = ( 20 , 5 )) if len ( model_names ) == 1 : middle = 0 else : middle = . 1 if kde : widths = [ 1.5 , middle , 1.5 ] else : widths = [ 1.5 , middle , 0 ] heights = [ 1.5 ] gs = gridspec . GridSpec ( 1 , 3 , width_ratios = widths , height_ratios = heights ) ax1 = plt . subplot ( gs [:, 0 ]) if kde : ax2 = plt . subplot ( gs [:, 2 ], sharex = ax1 ) # Precision-recall curve for color , model_name in zip ( cmap . colors , model_names ): ax1 . plot ( min_precisions [ model_name ], recall [ model_name ], color = color ) ax1 . set_ylim ( bottom = 0 , top = 1 ) ax1 . set_xlim ( left = 0 , right = 1 ) ax1 . spines [ 'right' ] . set_visible ( False ) ax1 . spines [ 'top' ] . set_visible ( False ) ax1 . set_xlabel ( r \"$\\bf {Precision} $\" + \" \\n (Minimum Similarity)\" ) ax1 . set_ylabel ( r \"$\\bf {Recall} $\" + \" \\n (Percentage Matched)\" ) # Similarity Histogram if kde : for color , model_name in zip ( cmap . colors , model_names ): sns . kdeplot ( matches [ model_name ][ \"Similarity\" ], fill = True , ax = ax2 , color = color ) ax2 . yaxis . set_label_position ( \"right\" ) ax2 . yaxis . tick_right () ax2 . set_xlabel ( r \"$\\bf {Similarity} $\" ) ax2 . set_ylabel ( \"\" ) ax2 . set_xlim ( left =- 0 , right = 1 ) plt . setp ([ ax2 ], title = 'Score Frequency - KDE' ) # Titles if len ( model_names ) == 1 and kde : fig . suptitle ( f 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) elif kde : fig . suptitle ( 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) else : fig . suptitle ( 'Precision-Recall Curve' , size = 20 , y = 1 , x = 0.45 ) # Custom Legend if len ( model_names ) > 1 : custom_lines = [ Line2D ([ 0 ], [ 0 ], color = color , lw = 4 ) for color , model_name in zip ( cmap . colors , model_names )] ax1 . legend ( custom_lines , model_names , bbox_to_anchor = ( 1.05 , . 61 , . 7 , . 902 ), loc = 3 , ncol = 1 , borderaxespad = 0. , frameon = True , fontsize = 10 ) if save_path : plt . savefig ( save_path , dpi = 300 )","title":"Metrics"},{"location":"api/metrics/#polyfuzzmetrics","text":"","title":"polyfuzz.metrics"},{"location":"api/metrics/#polyfuzz.metrics","text":"","title":"polyfuzz.metrics"},{"location":"api/metrics/#polyfuzz.metrics.precision_recall_curve","text":"Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters: Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for calculating precision, recall, and average precision required precision_steps float the incremental steps in minimum precision 0.01 Returns: Type Description Tuple[List[float], List[float], List[float]] min_precisions: minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step Source code in polyfuzz\\metrics.py def precision_recall_curve ( matches : pd . DataFrame , precision_steps : float = 0.01 ) -> Tuple [ List [ float ], List [ float ], List [ float ]]: \"\"\" Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision precision_steps: the incremental steps in minimum precision Returns: min_precisions: minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step \"\"\" min_precisions = list ( np . arange ( 0. , 1 + precision_steps , precision_steps )) average_precision = [] recall = [] similarities = matches . Similarity . values total = len ( matches ) for min_precision in min_precisions : selection = similarities [ similarities >= min_precision ] recall . append ( len ( selection ) / total ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = RuntimeWarning ) average_precision . append ( float ( np . mean ( selection ))) return min_precisions , recall , average_precision","title":"precision_recall_curve()"},{"location":"api/metrics/#polyfuzz.metrics.visualize_precision_recall","text":"Visualize the precision recall curve for one or more models Parameters: Name Type Description Default matches Mapping[str, pandas.core.frame.DataFrame] contains the columns From , To , and Similarity used for calculating precision, recall, and average precision per model required min_precisions Mapping[str, List[float]] minimum precision steps per model required recall Mapping[str, List[float]] recall per minimum precision step per model required kde bool whether to also visualize the kde plot True save_path str the path to save the resulting image to None Usage: visualize_precision_recall(matches, min_precisions, recall, save_path=\"data/results.png\") Source code in polyfuzz\\metrics.py def visualize_precision_recall ( matches : Mapping [ str , pd . DataFrame ], min_precisions : Mapping [ str , List [ float ]], recall : Mapping [ str , List [ float ]], kde : bool = True , save_path : str = None ): \"\"\" Visualize the precision recall curve for one or more models Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision per model min_precisions: minimum precision steps per model recall: recall per minimum precision step per model kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python visualize_precision_recall(matches, min_precisions, recall, save_path=\"data/results.png\") ``` \"\"\" SMALL_SIZE = 10 MEDIUM_SIZE = 12 BIGGER_SIZE = 14 plt . rc ( 'font' , size = SMALL_SIZE ) # controls default text sizes plt . rc ( 'axes' , titlesize = SMALL_SIZE ) # fontsize of the axes title plt . rc ( 'axes' , labelsize = MEDIUM_SIZE ) # fontsize of the x and y labels plt . rc ( 'xtick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'ytick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'legend' , fontsize = SMALL_SIZE ) # legend fontsize plt . rc ( 'figure' , titlesize = BIGGER_SIZE ) # fontsize of the figure title if not isinstance ( matches , dict ): matches = { \"Model\" : matches } min_precisions = { \"Model\" : min_precisions } recall = { \"Model\" : recall } # Create single dataset of similarity score for all models distribution_data = [( matches [ name ] . Similarity . values , [ name for _ in range ( len ( matches [ name ]))]) for name in matches . keys ()] distribution_data = pd . DataFrame ( np . hstack ( distribution_data ) . T , columns = [ \"Similarity\" , \"Model\" ]) distribution_data . Similarity = distribution_data . Similarity . astype ( float ) model_names = list ( matches . keys ()) # Create layout cmap = get_cmap ( 'Accent' ) fig = plt . figure ( figsize = ( 20 , 5 )) if len ( model_names ) == 1 : middle = 0 else : middle = . 1 if kde : widths = [ 1.5 , middle , 1.5 ] else : widths = [ 1.5 , middle , 0 ] heights = [ 1.5 ] gs = gridspec . GridSpec ( 1 , 3 , width_ratios = widths , height_ratios = heights ) ax1 = plt . subplot ( gs [:, 0 ]) if kde : ax2 = plt . subplot ( gs [:, 2 ], sharex = ax1 ) # Precision-recall curve for color , model_name in zip ( cmap . colors , model_names ): ax1 . plot ( min_precisions [ model_name ], recall [ model_name ], color = color ) ax1 . set_ylim ( bottom = 0 , top = 1 ) ax1 . set_xlim ( left = 0 , right = 1 ) ax1 . spines [ 'right' ] . set_visible ( False ) ax1 . spines [ 'top' ] . set_visible ( False ) ax1 . set_xlabel ( r \"$\\bf {Precision} $\" + \" \\n (Minimum Similarity)\" ) ax1 . set_ylabel ( r \"$\\bf {Recall} $\" + \" \\n (Percentage Matched)\" ) # Similarity Histogram if kde : for color , model_name in zip ( cmap . colors , model_names ): sns . kdeplot ( matches [ model_name ][ \"Similarity\" ], fill = True , ax = ax2 , color = color ) ax2 . yaxis . set_label_position ( \"right\" ) ax2 . yaxis . tick_right () ax2 . set_xlabel ( r \"$\\bf {Similarity} $\" ) ax2 . set_ylabel ( \"\" ) ax2 . set_xlim ( left =- 0 , right = 1 ) plt . setp ([ ax2 ], title = 'Score Frequency - KDE' ) # Titles if len ( model_names ) == 1 and kde : fig . suptitle ( f 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) elif kde : fig . suptitle ( 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) else : fig . suptitle ( 'Precision-Recall Curve' , size = 20 , y = 1 , x = 0.45 ) # Custom Legend if len ( model_names ) > 1 : custom_lines = [ Line2D ([ 0 ], [ 0 ], color = color , lw = 4 ) for color , model_name in zip ( cmap . colors , model_names )] ax1 . legend ( custom_lines , model_names , bbox_to_anchor = ( 1.05 , . 61 , . 7 , . 902 ), loc = 3 , ncol = 1 , borderaxespad = 0. , frameon = True , fontsize = 10 ) if save_path : plt . savefig ( save_path , dpi = 300 )","title":"visualize_precision_recall()"},{"location":"api/polyfuzz/","text":"polyfuzz.polyfuzz.PolyFuzz \u00b6 \u00b6 PolyFuzz class for Fuzzy string matching, grouping, and evaluation. Parameters: Name Type Description Default method the method(s) used for matching. For quick selection of models select one of the following: \"EditDistance\", \"TF-IDF\" or \"Embeddings\". If you want more control over the models above, pass in a model from polyfuzz.models. For examples, see usage below. required verbose Changes the verbosity of the model, Set to True if you want to track the stages of the model. required Usage: For basic, out-of-the-box usage, run the code below. You can replace \"TF-IDF\" with either \"EditDistance\" or \"Embeddings\" for quick access to these models: import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\") If you want more control over the String Matching models, you can load in these models separately: tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0, model_id=\"TF-IDF-Sklearn\") model = pf.PolyFuzz(tfidf) You can also select multiple models in order to compare performance: tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0, model_id=\"TF-IDF-Sklearn\") edit = EditDistance(n_jobs=-1) model = pf.PolyFuzz([tfidf, edit]) To use embedding models, please use Flair word embeddings: from flair.embeddings import WordEmbeddings, TransformerWordEmbeddings fasttext_embedding = WordEmbeddings('news') bert_embedding = TransformerWordEmbeddings('bert-base-multilingual-cased') embedding = Embeddings([fasttext_embedding, bert_embedding ], min_similarity=0.0) model = pf.PolyFuzz(embedding) get_cluster_mappings ( self , name = None ) \u00b6 Get the mappings from the To column to its respective column Source code in polyfuzz\\polyfuzz.py def get_cluster_mappings ( self , name : str = None ) -> Mapping [ str , int ]: \"\"\" Get the mappings from the `To` column to its respective column \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . cluster_mappings . values ())[ 0 ] elif len ( self . matches ) > 1 and name : return self . cluster_mappings [ name ] return self . cluster_mappings get_clusters ( self , model_id = None ) \u00b6 Get the groupings/clusters from a single model Parameters: Name Type Description Default model_id str the model id of the model if you have specified multiple models None Source code in polyfuzz\\polyfuzz.py def get_clusters ( self , model_id : str = None ) -> Mapping [ str , List [ str ]]: \"\"\" Get the groupings/clusters from a single model Arguments: model_id: the model id of the model if you have specified multiple models \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . clusters . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . clusters [ model_id ] return self . clusters get_ids ( self ) \u00b6 Get all model ids for easier access Source code in polyfuzz\\polyfuzz.py def get_ids ( self ) -> Union [ str , List [ str ], None ]: \"\"\" Get all model ids for easier access \"\"\" check_matches ( self ) if isinstance ( self . method , str ): return self . method elif isinstance ( self . method , Iterable ): return [ model . model_id for model in self . method ] return None get_matches ( self , model_id = None ) \u00b6 Get the matches from one or more models Source code in polyfuzz\\polyfuzz.py def get_matches ( self , model_id : str = None ) -> Union [ pd . DataFrame , Mapping [ str , pd . DataFrame ]]: \"\"\" Get the matches from one or more models\"\"\" check_matches ( self ) if len ( self . matches ) == 1 : return list ( self . matches . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . matches [ model_id ] return self . matches group ( self , model = None , link_min_similarity = 0.75 , group_all_strings = False ) \u00b6 From the matches, group the To matches together using single linkage Parameters: Name Type Description Default model Union[str, polyfuzz.models._base.BaseMatcher] you can choose one of the models in polyfuzz.models to be used as a grouper None link_min_similarity float the minimum similarity between strings before they are grouped in a single linkage fashion 0.75 group_all_strings bool if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. False Updates: self.matches: Adds a column Group that is the grouped version of the To column Source code in polyfuzz\\polyfuzz.py def group ( self , model : Union [ str , BaseMatcher ] = None , link_min_similarity : float = 0.75 , group_all_strings : bool = False ): \"\"\" From the matches, group the `To` matches together using single linkage Arguments: model: you can choose one of the models in `polyfuzz.models` to be used as a grouper link_min_similarity: the minimum similarity between strings before they are grouped in a single linkage fashion group_all_strings: if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. Updates: self.matches: Adds a column `Group` that is the grouped version of the `To` column \"\"\" check_matches ( self ) self . clusters = {} self . cluster_mappings = {} # Standard models - quick access if isinstance ( model , str ): if model in [ \"TF-IDF\" , \"TFIDF\" ]: model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: model = RapidFuzz () elif self . method in [ \"Embeddings\" , \"Embedding\" ]: model = Embeddings ( min_similarity = link_min_similarity ) else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" \"* Or None if you want to automatically use TF-IDF\" ) # Use TF-IDF if no model is specified elif not model : model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) # Group per model for name , match in self . matches . items (): self . _create_groups ( name , model , link_min_similarity , group_all_strings ) match ( self , from_list , to_list , top_n = 1 ) \u00b6 Match the from_list of strings to the to_list of strings with whatever models you have initialized Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required top_n int The number of matches you want returned. This is currently only implemented for polyfuzz.models.TFIDF and polyfuzz.models.Embeddings as they can computationally handle more comparisons. 1 !!! updates self.matches: A dictionary with the matches from all models, can be accessed with model.get_all_matches or model.get_match(\"TF-IDF\") Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) You can access the results matches with model.get_all_matches or a specific model with model.get_match(\"TF-IDF\") based on their model_id. Source code in polyfuzz\\polyfuzz.py def match ( self , from_list : List [ str ], to_list : List [ str ], top_n : int = 1 ): \"\"\" Match the from_list of strings to the to_list of strings with whatever models you have initialized Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to top_n: The number of matches you want returned. This is currently only implemented for `polyfuzz.models.TFIDF` and `polyfuzz.models.Embeddings` as they can computationally handle more comparisons. Updates: self.matches: A dictionary with the matches from all models, can be accessed with `model.get_all_matches` or `model.get_match(\"TF-IDF\")` Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can access the results matches with `model.get_all_matches` or a specific model with `model.get_match(\"TF-IDF\")` based on their model_id. \"\"\" # Standard models - quick access if isinstance ( self . method , str ): if self . method in [ \"TF-IDF\" , \"TFIDF\" ]: self . matches = { \"TF-IDF\" : TFIDF ( min_similarity = 0 , top_n = top_n ) . match ( from_list , to_list )} elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: self . matches = { \"EditDistance\" : RapidFuzz () . match ( from_list , to_list )} elif self . method in [ \"Embeddings\" , \"Embedding\" ]: self . matches = { \"Embeddings\" : Embeddings ( min_similarity = 0 , top_n = top_n ) . match ( from_list , to_list )} else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" ) logger . info ( f \"Ran model with model id = { self . method } \" ) # Custom models elif isinstance ( self . method , BaseMatcher ): self . matches = { self . method . model_id : self . method . match ( from_list , to_list )} logging . info ( f \"Ran model with model id = { self . method . model_id } \" ) # Multiple custom models elif isinstance ( self . method , Iterable ): self . _update_model_ids () self . matches = {} for model in self . method : self . matches [ model . model_id ] = model . match ( from_list , to_list ) logging . info ( f \"Ran model with model id = { model . model_id } \" ) return self visualize_precision_recall ( self , kde = False , save_path = None ) \u00b6 Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters: Name Type Description Default kde bool whether to also visualize the kde plot False save_path str the path to save the resulting image to None Usage: import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) model.visualize_precision_recall(save_path=\"results.png\") Source code in polyfuzz\\polyfuzz.py def visualize_precision_recall ( self , kde : bool = False , save_path : str = None ): \"\"\" Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) model.visualize_precision_recall(save_path=\"results.png\") ``` \"\"\" check_matches ( self ) self . min_precisions = {} self . recalls = {} self . average_precisions = {} for name , match in self . matches . items (): min_precision , recall , average_precision = precision_recall_curve ( match ) self . min_precisions [ name ] = min_precision self . recalls [ name ] = recall self . average_precisions [ name ] = average_precision visualize_precision_recall ( self . matches , self . min_precisions , self . recalls , kde , save_path )","title":"PolyFuzz"},{"location":"api/polyfuzz/#polyfuzzpolyfuzzpolyfuzz","text":"","title":"polyfuzz.polyfuzz.PolyFuzz"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz","text":"PolyFuzz class for Fuzzy string matching, grouping, and evaluation. Parameters: Name Type Description Default method the method(s) used for matching. For quick selection of models select one of the following: \"EditDistance\", \"TF-IDF\" or \"Embeddings\". If you want more control over the models above, pass in a model from polyfuzz.models. For examples, see usage below. required verbose Changes the verbosity of the model, Set to True if you want to track the stages of the model. required Usage: For basic, out-of-the-box usage, run the code below. You can replace \"TF-IDF\" with either \"EditDistance\" or \"Embeddings\" for quick access to these models: import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\") If you want more control over the String Matching models, you can load in these models separately: tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0, model_id=\"TF-IDF-Sklearn\") model = pf.PolyFuzz(tfidf) You can also select multiple models in order to compare performance: tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0, model_id=\"TF-IDF-Sklearn\") edit = EditDistance(n_jobs=-1) model = pf.PolyFuzz([tfidf, edit]) To use embedding models, please use Flair word embeddings: from flair.embeddings import WordEmbeddings, TransformerWordEmbeddings fasttext_embedding = WordEmbeddings('news') bert_embedding = TransformerWordEmbeddings('bert-base-multilingual-cased') embedding = Embeddings([fasttext_embedding, bert_embedding ], min_similarity=0.0) model = pf.PolyFuzz(embedding)","title":"polyfuzz.polyfuzz.PolyFuzz"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_cluster_mappings","text":"Get the mappings from the To column to its respective column Source code in polyfuzz\\polyfuzz.py def get_cluster_mappings ( self , name : str = None ) -> Mapping [ str , int ]: \"\"\" Get the mappings from the `To` column to its respective column \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . cluster_mappings . values ())[ 0 ] elif len ( self . matches ) > 1 and name : return self . cluster_mappings [ name ] return self . cluster_mappings","title":"get_cluster_mappings()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_clusters","text":"Get the groupings/clusters from a single model Parameters: Name Type Description Default model_id str the model id of the model if you have specified multiple models None Source code in polyfuzz\\polyfuzz.py def get_clusters ( self , model_id : str = None ) -> Mapping [ str , List [ str ]]: \"\"\" Get the groupings/clusters from a single model Arguments: model_id: the model id of the model if you have specified multiple models \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . clusters . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . clusters [ model_id ] return self . clusters","title":"get_clusters()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_ids","text":"Get all model ids for easier access Source code in polyfuzz\\polyfuzz.py def get_ids ( self ) -> Union [ str , List [ str ], None ]: \"\"\" Get all model ids for easier access \"\"\" check_matches ( self ) if isinstance ( self . method , str ): return self . method elif isinstance ( self . method , Iterable ): return [ model . model_id for model in self . method ] return None","title":"get_ids()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_matches","text":"Get the matches from one or more models Source code in polyfuzz\\polyfuzz.py def get_matches ( self , model_id : str = None ) -> Union [ pd . DataFrame , Mapping [ str , pd . DataFrame ]]: \"\"\" Get the matches from one or more models\"\"\" check_matches ( self ) if len ( self . matches ) == 1 : return list ( self . matches . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . matches [ model_id ] return self . matches","title":"get_matches()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.group","text":"From the matches, group the To matches together using single linkage Parameters: Name Type Description Default model Union[str, polyfuzz.models._base.BaseMatcher] you can choose one of the models in polyfuzz.models to be used as a grouper None link_min_similarity float the minimum similarity between strings before they are grouped in a single linkage fashion 0.75 group_all_strings bool if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. False Updates: self.matches: Adds a column Group that is the grouped version of the To column Source code in polyfuzz\\polyfuzz.py def group ( self , model : Union [ str , BaseMatcher ] = None , link_min_similarity : float = 0.75 , group_all_strings : bool = False ): \"\"\" From the matches, group the `To` matches together using single linkage Arguments: model: you can choose one of the models in `polyfuzz.models` to be used as a grouper link_min_similarity: the minimum similarity between strings before they are grouped in a single linkage fashion group_all_strings: if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. Updates: self.matches: Adds a column `Group` that is the grouped version of the `To` column \"\"\" check_matches ( self ) self . clusters = {} self . cluster_mappings = {} # Standard models - quick access if isinstance ( model , str ): if model in [ \"TF-IDF\" , \"TFIDF\" ]: model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: model = RapidFuzz () elif self . method in [ \"Embeddings\" , \"Embedding\" ]: model = Embeddings ( min_similarity = link_min_similarity ) else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" \"* Or None if you want to automatically use TF-IDF\" ) # Use TF-IDF if no model is specified elif not model : model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) # Group per model for name , match in self . matches . items (): self . _create_groups ( name , model , link_min_similarity , group_all_strings )","title":"group()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.match","text":"Match the from_list of strings to the to_list of strings with whatever models you have initialized Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required top_n int The number of matches you want returned. This is currently only implemented for polyfuzz.models.TFIDF and polyfuzz.models.Embeddings as they can computationally handle more comparisons. 1 !!! updates self.matches: A dictionary with the matches from all models, can be accessed with model.get_all_matches or model.get_match(\"TF-IDF\") Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) You can access the results matches with model.get_all_matches or a specific model with model.get_match(\"TF-IDF\") based on their model_id. Source code in polyfuzz\\polyfuzz.py def match ( self , from_list : List [ str ], to_list : List [ str ], top_n : int = 1 ): \"\"\" Match the from_list of strings to the to_list of strings with whatever models you have initialized Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to top_n: The number of matches you want returned. This is currently only implemented for `polyfuzz.models.TFIDF` and `polyfuzz.models.Embeddings` as they can computationally handle more comparisons. Updates: self.matches: A dictionary with the matches from all models, can be accessed with `model.get_all_matches` or `model.get_match(\"TF-IDF\")` Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can access the results matches with `model.get_all_matches` or a specific model with `model.get_match(\"TF-IDF\")` based on their model_id. \"\"\" # Standard models - quick access if isinstance ( self . method , str ): if self . method in [ \"TF-IDF\" , \"TFIDF\" ]: self . matches = { \"TF-IDF\" : TFIDF ( min_similarity = 0 , top_n = top_n ) . match ( from_list , to_list )} elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: self . matches = { \"EditDistance\" : RapidFuzz () . match ( from_list , to_list )} elif self . method in [ \"Embeddings\" , \"Embedding\" ]: self . matches = { \"Embeddings\" : Embeddings ( min_similarity = 0 , top_n = top_n ) . match ( from_list , to_list )} else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" ) logger . info ( f \"Ran model with model id = { self . method } \" ) # Custom models elif isinstance ( self . method , BaseMatcher ): self . matches = { self . method . model_id : self . method . match ( from_list , to_list )} logging . info ( f \"Ran model with model id = { self . method . model_id } \" ) # Multiple custom models elif isinstance ( self . method , Iterable ): self . _update_model_ids () self . matches = {} for model in self . method : self . matches [ model . model_id ] = model . match ( from_list , to_list ) logging . info ( f \"Ran model with model id = { model . model_id } \" ) return self","title":"match()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.visualize_precision_recall","text":"Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters: Name Type Description Default kde bool whether to also visualize the kde plot False save_path str the path to save the resulting image to None Usage: import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) model.visualize_precision_recall(save_path=\"results.png\") Source code in polyfuzz\\polyfuzz.py def visualize_precision_recall ( self , kde : bool = False , save_path : str = None ): \"\"\" Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) model.visualize_precision_recall(save_path=\"results.png\") ``` \"\"\" check_matches ( self ) self . min_precisions = {} self . recalls = {} self . average_precisions = {} for name , match in self . matches . items (): min_precision , recall , average_precision = precision_recall_curve ( match ) self . min_precisions [ name ] = min_precision self . recalls [ name ] = recall self . average_precisions [ name ] = average_precision visualize_precision_recall ( self . matches , self . min_precisions , self . recalls , kde , save_path )","title":"visualize_precision_recall()"},{"location":"api/models/base/","text":"polyfuzz.models.BaseMatcher \u00b6 \u00b6 The abstract BaseMatching to be modelled after for string matching match ( self , from_list , to_list ) \u00b6 Make sure you follow the same argument structure: Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns: Type Description DataFrame matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\" Source code in polyfuzz\\models\\_base.py @abstractmethod def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Make sure you follow the same argument structure: Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\" \"\"\" raise NotImplementedError ()","title":"BaseMatcher"},{"location":"api/models/base/#polyfuzzmodelsbasematcher","text":"","title":"polyfuzz.models.BaseMatcher"},{"location":"api/models/base/#polyfuzz.models._base.BaseMatcher","text":"The abstract BaseMatching to be modelled after for string matching","title":"polyfuzz.models._base.BaseMatcher"},{"location":"api/models/base/#polyfuzz.models._base.BaseMatcher.match","text":"Make sure you follow the same argument structure: Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns: Type Description DataFrame matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\" Source code in polyfuzz\\models\\_base.py @abstractmethod def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Make sure you follow the same argument structure: Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\" \"\"\" raise NotImplementedError ()","title":"match()"},{"location":"api/models/distance/","text":"polyfuzz.models.EditDistance \u00b6 \u00b6 Calculate the Edit Distance between lists of strings using any distance/similarity based scorer Parameters: Name Type Description Default n_jobs Nr of parallel processes, use -1 to use all cores required scorer The scorer function to be used to calculate the edit distance. This function should give back a float between 0 and 1, and work as follows: scorer(\"string_one\", \"string_two\") required model_id The name of the particular instance, used when comparing models required Usage: from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, scorer=fuzz.WRatio) match ( self , from_list , to_list ) \u00b6 Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns: Type Description DataFrame matches: The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) Source code in polyfuzz\\models\\_distance.py def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if from_list == to_list : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) if self . normalize : matches [ \"Similarity\" ] = ( matches [ \"Similarity\" ] - matches [ \"Similarity\" ] . min ()) / ( matches [ \"Similarity\" ] . max () - matches [ \"Similarity\" ] . min ()) return matches","title":"EditDistance"},{"location":"api/models/distance/#polyfuzzmodelseditdistance","text":"","title":"polyfuzz.models.EditDistance"},{"location":"api/models/distance/#polyfuzz.models._distance.EditDistance","text":"Calculate the Edit Distance between lists of strings using any distance/similarity based scorer Parameters: Name Type Description Default n_jobs Nr of parallel processes, use -1 to use all cores required scorer The scorer function to be used to calculate the edit distance. This function should give back a float between 0 and 1, and work as follows: scorer(\"string_one\", \"string_two\") required model_id The name of the particular instance, used when comparing models required Usage: from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, scorer=fuzz.WRatio)","title":"polyfuzz.models._distance.EditDistance"},{"location":"api/models/distance/#polyfuzz.models._distance.EditDistance.match","text":"Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns: Type Description DataFrame matches: The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) Source code in polyfuzz\\models\\_distance.py def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if from_list == to_list : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) if self . normalize : matches [ \"Similarity\" ] = ( matches [ \"Similarity\" ] - matches [ \"Similarity\" ] . min ()) / ( matches [ \"Similarity\" ] . max () - matches [ \"Similarity\" ] . min ()) return matches","title":"match()"},{"location":"api/models/embeddings/","text":"polyfuzz.models.Embeddings \u00b6 \u00b6 Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters: Name Type Description Default embedding_method list of Flair embeddings to use required min_similarity The minimum similarity between strings, otherwise return 0 similarity required top_n The number of best matches you want returned required cosine_method The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory required model_id The name of the particular instance, used when comparing models required Usage: model = Embeddings(min_similarity=0.5) Or if you want a custom model to be used and it is a word embedding model, pass it in as a list: embedding_model = WordEmbeddings('news') model = Embeddings([embeddings_model], min_similarity=0.5) As you might have guessed, you can pass along multiple word embedding models and the results will be averaged: fasttext_embedding = WordEmbeddings('news') glove_embedding = WordEmbeddings('glove') bert_embedding = TransformerWordEmbeddings('bert-base-multilingual-cased') model = Embeddings([glove_embedding, fasttext_embedding, bert_embedding ], min_similarity=0.5) match ( self , from_list , to_list , embeddings_from = None , embeddings_to = None ) \u00b6 Matches the two lists of strings to each other and returns the best mapping Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None Returns: Type Description DataFrame matches: The best matches between the lists of strings Usage: model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) Source code in polyfuzz\\models\\_embeddings.py def match ( self , from_list : List [ str ], to_list : List [ str ], embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if not isinstance ( embeddings_from , np . ndarray ): embeddings_from = self . _embed ( from_list ) if not isinstance ( embeddings_to , np . ndarray ): embeddings_to = self . _embed ( to_list ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) return matches","title":"Embeddings"},{"location":"api/models/embeddings/#polyfuzzmodelsembeddings","text":"","title":"polyfuzz.models.Embeddings"},{"location":"api/models/embeddings/#polyfuzz.models._embeddings.Embeddings","text":"Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters: Name Type Description Default embedding_method list of Flair embeddings to use required min_similarity The minimum similarity between strings, otherwise return 0 similarity required top_n The number of best matches you want returned required cosine_method The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory required model_id The name of the particular instance, used when comparing models required Usage: model = Embeddings(min_similarity=0.5) Or if you want a custom model to be used and it is a word embedding model, pass it in as a list: embedding_model = WordEmbeddings('news') model = Embeddings([embeddings_model], min_similarity=0.5) As you might have guessed, you can pass along multiple word embedding models and the results will be averaged: fasttext_embedding = WordEmbeddings('news') glove_embedding = WordEmbeddings('glove') bert_embedding = TransformerWordEmbeddings('bert-base-multilingual-cased') model = Embeddings([glove_embedding, fasttext_embedding, bert_embedding ], min_similarity=0.5)","title":"polyfuzz.models._embeddings.Embeddings"},{"location":"api/models/embeddings/#polyfuzz.models._embeddings.Embeddings.match","text":"Matches the two lists of strings to each other and returns the best mapping Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None Returns: Type Description DataFrame matches: The best matches between the lists of strings Usage: model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) Source code in polyfuzz\\models\\_embeddings.py def match ( self , from_list : List [ str ], to_list : List [ str ], embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if not isinstance ( embeddings_from , np . ndarray ): embeddings_from = self . _embed ( from_list ) if not isinstance ( embeddings_to , np . ndarray ): embeddings_to = self . _embed ( to_list ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) return matches","title":"match()"},{"location":"api/models/matches/","text":"polyfuzz.models.cosine_similarity \u00b6 \u00b6 Calculate similarity between two matrices/vectors and return best matches Parameters: Name Type Description Default from_vector ndarray the matrix or vector representing the embedded strings to map from required to_vector ndarray the matrix or vector representing the embedded strings to map to required from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' Returns: Type Description DataFrame matches: The best matches between the lists of strings Usage: Make sure to fill the to_vector and from_vector with vector representations of to_list and from_list respectively: from polyfuzz.models import extract_best_matches indices, similarity = extract_best_matches(from_vector, to_vector, method=\"sparse\") Source code in polyfuzz\\models\\_utils.py def cosine_similarity ( from_vector : np . ndarray , to_vector : np . ndarray , from_list : List [ str ], to_list : List [ str ], min_similarity : float = 0.75 , top_n : int = 1 , method : str = \"sparse\" ) -> pd . DataFrame : \"\"\" Calculate similarity between two matrices/vectors and return best matches Arguments: from_vector: the matrix or vector representing the embedded strings to map from to_vector: the matrix or vector representing the embedded strings to map to from_list: The list from which you want mappings to_list: The list where you want to map to min_similarity: The minimum similarity between strings, otherwise return 0 similarity top_n: The number of best matches you want returned method: The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory Returns: matches: The best matches between the lists of strings Usage: Make sure to fill the `to_vector` and `from_vector` with vector representations of `to_list` and `from_list` respectively: ```python from polyfuzz.models import extract_best_matches indices, similarity = extract_best_matches(from_vector, to_vector, method=\"sparse\") ``` \"\"\" if top_n > len ( set ( to_list )): top_n = len ( set ( to_list )) # Slower but uses less memory if method == \"knn\" : if from_list == to_list : knn = NearestNeighbors ( n_neighbors = top_n + 1 , n_jobs =- 1 , metric = 'cosine' ) . fit ( to_vector ) distances , indices = knn . kneighbors ( from_vector ) distances = distances [:, 1 :] indices = indices [:, 1 :] else : knn = NearestNeighbors ( n_neighbors = top_n , n_jobs =- 1 , metric = 'cosine' ) . fit ( to_vector ) distances , indices = knn . kneighbors ( from_vector ) similarities = [ np . round ( 1 - distances [:, i ], 3 ) for i in range ( distances . shape [ 1 ])] # Fast, but does has some installation issues elif _HAVE_SPARSE_DOT and method == \"sparse\" : if isinstance ( to_vector , np . ndarray ): to_vector = csr_matrix ( to_vector ) if isinstance ( from_vector , np . ndarray ): from_vector = csr_matrix ( from_vector ) # There is a bug with awesome_cossim_topn that when to_vector and from_vector # have the same shape, setting topn to 1 does not work. Apparently, you need # to it at least to 2 for it to work similarity_matrix = awesome_cossim_topn ( from_vector , to_vector . T , top_n + 1 , min_similarity ) if from_list == to_list : similarity_matrix = similarity_matrix . tolil () similarity_matrix . setdiag ( 0. ) similarity_matrix = similarity_matrix . tocsr () indices = _top_n_idx_sparse ( similarity_matrix , top_n ) similarities = _top_n_similarities_sparse ( similarity_matrix , indices ) indices = np . array ( np . nan_to_num ( np . array ( indices , dtype = np . float ), nan = 0 ), dtype = np . int ) # Faster than knn and slower than sparse but uses more memory else : similarity_matrix = scikit_cosine_similarity ( from_vector , to_vector ) if from_list == to_list : np . fill_diagonal ( similarity_matrix , 0 ) indices = np . flip ( np . argsort ( similarity_matrix , axis =- 1 ), axis = 1 )[:, : top_n ] similarities = np . flip ( np . sort ( similarity_matrix , axis =- 1 ), axis = 1 )[:, : top_n ] similarities = [ np . round ( similarities [:, i ], 3 ) for i in range ( similarities . shape [ 1 ])] # Convert results to df columns = ([ \"From\" ] + [ \"To\" if i == 0 else f \"To_ { i + 1 } \" for i in range ( top_n )] + [ \"Similarity\" if i == 0 else f \"Similarity_ { i + 1 } \" for i in range ( top_n )]) matches = [[ to_list [ idx ] for idx in indices [:, i ]] for i in range ( indices . shape [ 1 ])] matches = pd . DataFrame ( np . vstack (([ from_list ], matches , similarities )) . T , columns = columns ) # Update column order columns = [[ \"From\" , \"To\" , \"Similarity\" ]] + [[ f \"To_ { i + 2 } \" , f \"Similarity_ { i + 2 } \" ] for i in range (( top_n - 1 ))] matches = matches . loc [:, [ title for column in columns for title in column ]] # Update types for column in matches . columns : if \"Similarity\" in column : matches [ column ] = matches [ column ] . astype ( float ) matches . loc [ matches [ column ] < 0.001 , column ] = float ( 0 ) matches . loc [ matches [ column ] < 0.001 , column . replace ( \"Similarity\" , \"To\" )] = None return matches","title":"CosineSimilarity"},{"location":"api/models/matches/#polyfuzzmodelscosine_similarity","text":"","title":"polyfuzz.models.cosine_similarity"},{"location":"api/models/matches/#polyfuzz.models._utils.cosine_similarity","text":"Calculate similarity between two matrices/vectors and return best matches Parameters: Name Type Description Default from_vector ndarray the matrix or vector representing the embedded strings to map from required to_vector ndarray the matrix or vector representing the embedded strings to map to required from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' Returns: Type Description DataFrame matches: The best matches between the lists of strings Usage: Make sure to fill the to_vector and from_vector with vector representations of to_list and from_list respectively: from polyfuzz.models import extract_best_matches indices, similarity = extract_best_matches(from_vector, to_vector, method=\"sparse\") Source code in polyfuzz\\models\\_utils.py def cosine_similarity ( from_vector : np . ndarray , to_vector : np . ndarray , from_list : List [ str ], to_list : List [ str ], min_similarity : float = 0.75 , top_n : int = 1 , method : str = \"sparse\" ) -> pd . DataFrame : \"\"\" Calculate similarity between two matrices/vectors and return best matches Arguments: from_vector: the matrix or vector representing the embedded strings to map from to_vector: the matrix or vector representing the embedded strings to map to from_list: The list from which you want mappings to_list: The list where you want to map to min_similarity: The minimum similarity between strings, otherwise return 0 similarity top_n: The number of best matches you want returned method: The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory Returns: matches: The best matches between the lists of strings Usage: Make sure to fill the `to_vector` and `from_vector` with vector representations of `to_list` and `from_list` respectively: ```python from polyfuzz.models import extract_best_matches indices, similarity = extract_best_matches(from_vector, to_vector, method=\"sparse\") ``` \"\"\" if top_n > len ( set ( to_list )): top_n = len ( set ( to_list )) # Slower but uses less memory if method == \"knn\" : if from_list == to_list : knn = NearestNeighbors ( n_neighbors = top_n + 1 , n_jobs =- 1 , metric = 'cosine' ) . fit ( to_vector ) distances , indices = knn . kneighbors ( from_vector ) distances = distances [:, 1 :] indices = indices [:, 1 :] else : knn = NearestNeighbors ( n_neighbors = top_n , n_jobs =- 1 , metric = 'cosine' ) . fit ( to_vector ) distances , indices = knn . kneighbors ( from_vector ) similarities = [ np . round ( 1 - distances [:, i ], 3 ) for i in range ( distances . shape [ 1 ])] # Fast, but does has some installation issues elif _HAVE_SPARSE_DOT and method == \"sparse\" : if isinstance ( to_vector , np . ndarray ): to_vector = csr_matrix ( to_vector ) if isinstance ( from_vector , np . ndarray ): from_vector = csr_matrix ( from_vector ) # There is a bug with awesome_cossim_topn that when to_vector and from_vector # have the same shape, setting topn to 1 does not work. Apparently, you need # to it at least to 2 for it to work similarity_matrix = awesome_cossim_topn ( from_vector , to_vector . T , top_n + 1 , min_similarity ) if from_list == to_list : similarity_matrix = similarity_matrix . tolil () similarity_matrix . setdiag ( 0. ) similarity_matrix = similarity_matrix . tocsr () indices = _top_n_idx_sparse ( similarity_matrix , top_n ) similarities = _top_n_similarities_sparse ( similarity_matrix , indices ) indices = np . array ( np . nan_to_num ( np . array ( indices , dtype = np . float ), nan = 0 ), dtype = np . int ) # Faster than knn and slower than sparse but uses more memory else : similarity_matrix = scikit_cosine_similarity ( from_vector , to_vector ) if from_list == to_list : np . fill_diagonal ( similarity_matrix , 0 ) indices = np . flip ( np . argsort ( similarity_matrix , axis =- 1 ), axis = 1 )[:, : top_n ] similarities = np . flip ( np . sort ( similarity_matrix , axis =- 1 ), axis = 1 )[:, : top_n ] similarities = [ np . round ( similarities [:, i ], 3 ) for i in range ( similarities . shape [ 1 ])] # Convert results to df columns = ([ \"From\" ] + [ \"To\" if i == 0 else f \"To_ { i + 1 } \" for i in range ( top_n )] + [ \"Similarity\" if i == 0 else f \"Similarity_ { i + 1 } \" for i in range ( top_n )]) matches = [[ to_list [ idx ] for idx in indices [:, i ]] for i in range ( indices . shape [ 1 ])] matches = pd . DataFrame ( np . vstack (([ from_list ], matches , similarities )) . T , columns = columns ) # Update column order columns = [[ \"From\" , \"To\" , \"Similarity\" ]] + [[ f \"To_ { i + 2 } \" , f \"Similarity_ { i + 2 } \" ] for i in range (( top_n - 1 ))] matches = matches . loc [:, [ title for column in columns for title in column ]] # Update types for column in matches . columns : if \"Similarity\" in column : matches [ column ] = matches [ column ] . astype ( float ) matches . loc [ matches [ column ] < 0.001 , column ] = float ( 0 ) matches . loc [ matches [ column ] < 0.001 , column . replace ( \"Similarity\" , \"To\" )] = None return matches","title":"polyfuzz.models._utils.cosine_similarity"},{"location":"api/models/rapidfuzz/","text":"polyfuzz.models.RapidFuzz \u00b6 \u00b6 Calculate the Edit Distance between lists of strings using RapidFuzz's process function We are using RapidFuzz instead of FuzzyWuzzy since it is much faster and does not require the more restrictive GPL license Parameters: Name Type Description Default n_jobs Nr of parallel processes, use -1 to use all cores required score_cutoff The minimum similarity for which to return a good match. Should be between 0 and 1. required scorer The scorer function to be used to calculate the edit distance Options: * fuzz.ratio * fuzz.partial_ratio * fuzz.token_sort_ratio * fuzz.partial_token_sort_ratio * fuzz.token_set_ratio * fuzz.partial_token_set_ratio * fuzz.token_ratio * fuzz.partial_token_ratio * fuzz.WRation * fuzz.QRatio See https://maxbachmann.github.io/rapidfuzz/usage/fuzz/ for an extensive description of the scoring methods. required model_id The name of the particular instance, used when comparing models required Usage: from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) match ( self , from_list , to_list ) \u00b6 Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns: Type Description DataFrame matches: The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) Source code in polyfuzz\\models\\_rapidfuzz.py def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if from_list == to_list : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) return matches","title":"RapidFuzzy"},{"location":"api/models/rapidfuzz/#polyfuzzmodelsrapidfuzz","text":"","title":"polyfuzz.models.RapidFuzz"},{"location":"api/models/rapidfuzz/#polyfuzz.models._rapidfuzz.RapidFuzz","text":"Calculate the Edit Distance between lists of strings using RapidFuzz's process function We are using RapidFuzz instead of FuzzyWuzzy since it is much faster and does not require the more restrictive GPL license Parameters: Name Type Description Default n_jobs Nr of parallel processes, use -1 to use all cores required score_cutoff The minimum similarity for which to return a good match. Should be between 0 and 1. required scorer The scorer function to be used to calculate the edit distance Options: * fuzz.ratio * fuzz.partial_ratio * fuzz.token_sort_ratio * fuzz.partial_token_sort_ratio * fuzz.token_set_ratio * fuzz.partial_token_set_ratio * fuzz.token_ratio * fuzz.partial_token_ratio * fuzz.WRation * fuzz.QRatio See https://maxbachmann.github.io/rapidfuzz/usage/fuzz/ for an extensive description of the scoring methods. required model_id The name of the particular instance, used when comparing models required Usage: from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio)","title":"polyfuzz.models._rapidfuzz.RapidFuzz"},{"location":"api/models/rapidfuzz/#polyfuzz.models._rapidfuzz.RapidFuzz.match","text":"Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns: Type Description DataFrame matches: The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) Source code in polyfuzz\\models\\_rapidfuzz.py def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if from_list == to_list : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) return matches","title":"match()"},{"location":"api/models/tfidf/","text":"polyfuzz.models.TFIDF \u00b6 \u00b6 A character based n-gram TF-IDF to approximate edit distance We turn a string into, typically of length 3, n-grams. For example, using 3-grams of the \"hotel\" we get ['hot', 'ote', 'tel']. These are then used as input for a TfidfVectorizer in order to create a vector for each word. Then, we simply apply cosine similarity through k-NN Parameters: Name Type Description Default n_gram_range The n_gram_range on a character-level required clean_string Whether to clean the string such that only alphanumerical characters are kept required min_similarity The minimum similarity between strings, otherwise return 0 similarity required top_n The number of matches you want returned required cosine_method The method/package for calculating the cosine similarity. Options: * sparse * sklearn * knn sparse is the fastest and most memory efficient but requires a package that might be difficult to install sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory required model_id The name of the particular instance, used when comparing models required Usage: from polymatcher.models import TFIDF model = TFIDF(n_gram_range=(3, 3), clean_string=True, use_knn=False) match ( self , from_list , to_list ) \u00b6 Match two lists of strings to each other and return the most similar strings Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns: Type Description DataFrame matches: The best matches between the lists of strings Usage: from polymatcher.models import TFIDF model = TFIDF() matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) Source code in polyfuzz\\models\\_tfidf.py def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Match two lists of strings to each other and return the most similar strings Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from polymatcher.models import TFIDF model = TFIDF() matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" tf_idf_from , tf_idf_to = self . _extract_tf_idf ( from_list , to_list ) matches = cosine_similarity ( tf_idf_from , tf_idf_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) return matches","title":"TFIDF"},{"location":"api/models/tfidf/#polyfuzzmodelstfidf","text":"","title":"polyfuzz.models.TFIDF"},{"location":"api/models/tfidf/#polyfuzz.models._tfidf.TFIDF","text":"A character based n-gram TF-IDF to approximate edit distance We turn a string into, typically of length 3, n-grams. For example, using 3-grams of the \"hotel\" we get ['hot', 'ote', 'tel']. These are then used as input for a TfidfVectorizer in order to create a vector for each word. Then, we simply apply cosine similarity through k-NN Parameters: Name Type Description Default n_gram_range The n_gram_range on a character-level required clean_string Whether to clean the string such that only alphanumerical characters are kept required min_similarity The minimum similarity between strings, otherwise return 0 similarity required top_n The number of matches you want returned required cosine_method The method/package for calculating the cosine similarity. Options: * sparse * sklearn * knn sparse is the fastest and most memory efficient but requires a package that might be difficult to install sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory required model_id The name of the particular instance, used when comparing models required Usage: from polymatcher.models import TFIDF model = TFIDF(n_gram_range=(3, 3), clean_string=True, use_knn=False)","title":"polyfuzz.models._tfidf.TFIDF"},{"location":"api/models/tfidf/#polyfuzz.models._tfidf.TFIDF.match","text":"Match two lists of strings to each other and return the most similar strings Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required Returns: Type Description DataFrame matches: The best matches between the lists of strings Usage: from polymatcher.models import TFIDF model = TFIDF() matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) Source code in polyfuzz\\models\\_tfidf.py def match ( self , from_list : List [ str ], to_list : List [ str ]) -> pd . DataFrame : \"\"\" Match two lists of strings to each other and return the most similar strings Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from polymatcher.models import TFIDF model = TFIDF() matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" tf_idf_from , tf_idf_to = self . _extract_tf_idf ( from_list , to_list ) matches = cosine_similarity ( tf_idf_from , tf_idf_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) return matches","title":"match()"},{"location":"tutorial/basematcher/basematcher/","text":"Custom Models \u00b6 Although PolyFuzz has several models implemented, what if you have developed your own? What if you want a different similarity/distance measure that is not defined in PolyFuzz? That is where custom models come in. If you follow the structure of PolyFuzz's BaseMatcher you can quickly implement any model you would like. You simply create a class using BaseMatcher , make sure it has a function match that inputs two lists and outputs a pandas dataframe. That's it! We start by creating our own model that implements the ratio similarity measure from RapidFuzz: import numpy as np import pandas as pd from rapidfuzz import fuzz from polyfuzz import PolyFuzz from polyfuzz.models import BaseMatcher class MyModel(BaseMatcher): def match(self, from_list, to_list): # Calculate distances matches = [[fuzz.ratio(from_string, to_string) / 100 for to_string in to_list] for from_string in from_list] # Get best matches mappings = [to_list[index] for index in np.argmax(matches, axis=1)] scores = np.max(matches, axis=1) # Prepare dataframe matches = pd.DataFrame({'From': from_list, 'To': mappings, 'Similarity': scores}) return matches MyModel can now be used within PolyFuzz and runs like every other model: from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] custom_matcher = MyModel() model = PolyFuzz(custom_matcher).match(from_list, to_list) Now we can visualize the results: model.visualize_precision_recall(kde=True)","title":"Custom Models"},{"location":"tutorial/basematcher/basematcher/#custom-models","text":"Although PolyFuzz has several models implemented, what if you have developed your own? What if you want a different similarity/distance measure that is not defined in PolyFuzz? That is where custom models come in. If you follow the structure of PolyFuzz's BaseMatcher you can quickly implement any model you would like. You simply create a class using BaseMatcher , make sure it has a function match that inputs two lists and outputs a pandas dataframe. That's it! We start by creating our own model that implements the ratio similarity measure from RapidFuzz: import numpy as np import pandas as pd from rapidfuzz import fuzz from polyfuzz import PolyFuzz from polyfuzz.models import BaseMatcher class MyModel(BaseMatcher): def match(self, from_list, to_list): # Calculate distances matches = [[fuzz.ratio(from_string, to_string) / 100 for to_string in to_list] for from_string in from_list] # Get best matches mappings = [to_list[index] for index in np.argmax(matches, axis=1)] scores = np.max(matches, axis=1) # Prepare dataframe matches = pd.DataFrame({'From': from_list, 'To': mappings, 'Similarity': scores}) return matches MyModel can now be used within PolyFuzz and runs like every other model: from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] custom_matcher = MyModel() model = PolyFuzz(custom_matcher).match(from_list, to_list) Now we can visualize the results: model.visualize_precision_recall(kde=True)","title":"Custom Models"},{"location":"tutorial/datasets/datasets/","text":"Datasets \u00b6 There are two datasets prepared for you to play around with: * Company Names * Movie Titles Movie Titles \u00b6 This data is retrieved from: https://www.kaggle.com/stefanoleone992/imdb-extensive-dataset https://www.kaggle.com/shivamb/netflix-shows It contains Netflix and IMDB movie titles that can be matched against each other. Where IMDB has 80852 movie titles and Netflix has 6172 movie titles. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_movie_titles data = load_movie_titles() model = PolyFuzz(\"TF-IDF\").match(data[\"Netflix\"], data[\"IMDB\"]) Company Names \u00b6 This data is retrieved from https://www.kaggle.com/dattapiy/sec-edgar-companies-list?select=sec__edgar_company_info.csv and contains 100_000 company names to be matched against each other. This is a different use case than what you have typically seen so far. We often see two different lists compared with each other. Here, you can use this dataset to compare the company names with themselves in order to clean them up. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_company_names data = load_company_names() model = PolyFuzz(\"TF-IDF\").match(data, data) PolyFuzz will recognize that the lists are similar and that you are looking to match the titles with themselves. It will ignore any comparison a string has with itself, otherwise everything will get mapped to itself.","title":"Datasets"},{"location":"tutorial/datasets/datasets/#datasets","text":"There are two datasets prepared for you to play around with: * Company Names * Movie Titles","title":"Datasets"},{"location":"tutorial/datasets/datasets/#movie-titles","text":"This data is retrieved from: https://www.kaggle.com/stefanoleone992/imdb-extensive-dataset https://www.kaggle.com/shivamb/netflix-shows It contains Netflix and IMDB movie titles that can be matched against each other. Where IMDB has 80852 movie titles and Netflix has 6172 movie titles. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_movie_titles data = load_movie_titles() model = PolyFuzz(\"TF-IDF\").match(data[\"Netflix\"], data[\"IMDB\"])","title":"Movie Titles"},{"location":"tutorial/datasets/datasets/#company-names","text":"This data is retrieved from https://www.kaggle.com/dattapiy/sec-edgar-companies-list?select=sec__edgar_company_info.csv and contains 100_000 company names to be matched against each other. This is a different use case than what you have typically seen so far. We often see two different lists compared with each other. Here, you can use this dataset to compare the company names with themselves in order to clean them up. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_company_names data = load_company_names() model = PolyFuzz(\"TF-IDF\").match(data, data) PolyFuzz will recognize that the lists are similar and that you are looking to match the titles with themselves. It will ignore any comparison a string has with itself, otherwise everything will get mapped to itself.","title":"Company Names"},{"location":"tutorial/grouper/grouper/","text":"Custom Grouper \u00b6 The basic grouper is a TF-IDF implementation that uses single linkage to group the strings you mapped to together. With the customizability philosophy of PolyFuzz in mind it is not unexpected that you can also use any of the models, and even custom models, as your grouper! Here, we use Edit Distance instead of TF-IDF to group the strings we mapped to: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] model = PolyFuzz(\"TF-IDF\").match(from_list, to_list) # Custom grouper base_edit_grouper = EditDistance(n_jobs=1) model.group(base_edit_grouper) And that is it! We have now grouped our matches we mapped to together using Edit Distance instead of TF-IDF.","title":"Custom Grouper"},{"location":"tutorial/grouper/grouper/#custom-grouper","text":"The basic grouper is a TF-IDF implementation that uses single linkage to group the strings you mapped to together. With the customizability philosophy of PolyFuzz in mind it is not unexpected that you can also use any of the models, and even custom models, as your grouper! Here, we use Edit Distance instead of TF-IDF to group the strings we mapped to: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] model = PolyFuzz(\"TF-IDF\").match(from_list, to_list) # Custom grouper base_edit_grouper = EditDistance(n_jobs=1) model.group(base_edit_grouper) And that is it! We have now grouped our matches we mapped to together using Edit Distance instead of TF-IDF.","title":"Custom Grouper"},{"location":"tutorial/models/models/","text":"Models \u00b6 Currently, the following models are implemented in PolyFuzz: 1. TF-IDF 2. EditDistance with RapidFuzz 3. FastText and GloVe 4. \ud83e\udd17 Transformers With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzzy. All models listed above can be found in polyfuzz.models and can be used to create and compare different matchers. TF-IDF \u00b6 Although the terms in TF-IDF are usually words, we are going to be using TF-IDF on a character-level. We will be extracting n-grams from a string and count the frequency of these n-grams across all input strings. For example, with 3-grams, the word \"hotel\" can be defined as \"hot\", \"ote\", and \"tel\". After generating the n-grams and applying TF-IDF on these \"terms\", we can use cosine similarity to compare the generated TF-IDF vectors against each other. We simply load in TFIDF from polyfuzz.models and pass it to our PolyFuzz instance: from polyfuzz.models import TFIDF from polyfuzz import PolyFuzz from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0, matcher_id=\"TF-IDF\") model = PolyFuzz(tfidf).match(from_list, to_list) and that's it! You can play around with the TFIDF matcher until you get the results you are looking for. Note that if you increase the min_similarity there is a chance that some strings will not be matched at all. EditDistance \u00b6 There are many edit distance functions one could use and the EditDistance model from polyfuzz.models allows you to pass in any distance function. As long as that distance function takes in two strings and spits out a float, you can pass anything! In the example below, we are going to be using Jaro Winkler Similarity from the jellyfish package to create our custom scorer: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from jellyfish import jaro_winkler_similarity from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] jellyfish_matcher = EditDistance(n_jobs=1, scorer=jaro_winkler_similarity) model = PolyFuzz(jellyfish_matcher).match(from_list, to_list) RapidFuzz \u00b6 Edit distance measures are typically quite slow. Moreover, the one that is heavily used, fuzzywuzzy , has a very restrictive licence (GPL). Instead, I decided to create a RapidFuzz matcher which is a fast implementation of fuzzywuzzy and has a less restrictive licence (MIT): from polyfuzz import PolyFuzz from polyfuzz.models import RapidFuzz from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] rapidfuzz_matcher = RapidFuzz(n_jobs=1) model = PolyFuzz(rapidfuzz_matcher).match(from_list, to_list) Embeddings \u00b6 With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . The embeddings that are created are compared with cosine similarity in order to understand how similar the created embeddings are to each other. We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzzy: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] bert = TransformerWordEmbeddings('bert-base-multilingual-cased') bert_matcher = Embeddings(bert, min_similarity=0) models = PolyFuzz(bert_matcher).match(from_list, to_list) Flair allows you to use pool word embeddings to create more powerful word embeddings. Below, we pool FastText and BERT to create a single embedding representation from which we can calculate the similarity between strings: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings, WordEmbeddings from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] bert = TransformerWordEmbeddings('bert-base-multilingual-cased') bert_matcher = Embeddings(bert, min_similarity=0) fasttext = WordEmbeddings('en-crawl') fasttext_matcher = Embeddings(fasttext, min_similarity=0) matchers = [bert_matcher, fasttext_matcher] models = PolyFuzz(matchers).match(from_list, to_list) Using Multiple Models \u00b6 from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance, TFIDF, Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] bert = TransformerWordEmbeddings('bert-base-multilingual-cased') bert_matcher = Embeddings(bert, min_similarity=0, matcher_id=\"BERT\") tfidf_matcher = TFIDF(min_similarity=0) edit_matcher = EditDistance() matchers = [bert_matcher, tfidf_matcher, edit_matcher] models = PolyFuzz(matchers).match(from_list, to_list) To access the results, we again can call get_matches but since we have multiple models we get back a dictionary of dataframes back. In order to access the results of a specific model, call get_matches with the correct id: >>> models.get_matches(\"BERT\") From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.928045 3 recal apples 0.825268 4 house mouse 0.887524 5 similarity mouse 0.791548 Finally, visualize the results to compare the models: models.visualize_precision_recall(kde=True)","title":"Models"},{"location":"tutorial/models/models/#models","text":"Currently, the following models are implemented in PolyFuzz: 1. TF-IDF 2. EditDistance with RapidFuzz 3. FastText and GloVe 4. \ud83e\udd17 Transformers With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzzy. All models listed above can be found in polyfuzz.models and can be used to create and compare different matchers.","title":"Models"},{"location":"tutorial/models/models/#tf-idf","text":"Although the terms in TF-IDF are usually words, we are going to be using TF-IDF on a character-level. We will be extracting n-grams from a string and count the frequency of these n-grams across all input strings. For example, with 3-grams, the word \"hotel\" can be defined as \"hot\", \"ote\", and \"tel\". After generating the n-grams and applying TF-IDF on these \"terms\", we can use cosine similarity to compare the generated TF-IDF vectors against each other. We simply load in TFIDF from polyfuzz.models and pass it to our PolyFuzz instance: from polyfuzz.models import TFIDF from polyfuzz import PolyFuzz from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0, matcher_id=\"TF-IDF\") model = PolyFuzz(tfidf).match(from_list, to_list) and that's it! You can play around with the TFIDF matcher until you get the results you are looking for. Note that if you increase the min_similarity there is a chance that some strings will not be matched at all.","title":"TF-IDF"},{"location":"tutorial/models/models/#editdistance","text":"There are many edit distance functions one could use and the EditDistance model from polyfuzz.models allows you to pass in any distance function. As long as that distance function takes in two strings and spits out a float, you can pass anything! In the example below, we are going to be using Jaro Winkler Similarity from the jellyfish package to create our custom scorer: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from jellyfish import jaro_winkler_similarity from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] jellyfish_matcher = EditDistance(n_jobs=1, scorer=jaro_winkler_similarity) model = PolyFuzz(jellyfish_matcher).match(from_list, to_list)","title":"EditDistance"},{"location":"tutorial/models/models/#rapidfuzz","text":"Edit distance measures are typically quite slow. Moreover, the one that is heavily used, fuzzywuzzy , has a very restrictive licence (GPL). Instead, I decided to create a RapidFuzz matcher which is a fast implementation of fuzzywuzzy and has a less restrictive licence (MIT): from polyfuzz import PolyFuzz from polyfuzz.models import RapidFuzz from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] rapidfuzz_matcher = RapidFuzz(n_jobs=1) model = PolyFuzz(rapidfuzz_matcher).match(from_list, to_list)","title":"RapidFuzz"},{"location":"tutorial/models/models/#embeddings","text":"With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . The embeddings that are created are compared with cosine similarity in order to understand how similar the created embeddings are to each other. We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzzy: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] bert = TransformerWordEmbeddings('bert-base-multilingual-cased') bert_matcher = Embeddings(bert, min_similarity=0) models = PolyFuzz(bert_matcher).match(from_list, to_list) Flair allows you to use pool word embeddings to create more powerful word embeddings. Below, we pool FastText and BERT to create a single embedding representation from which we can calculate the similarity between strings: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings, WordEmbeddings from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] bert = TransformerWordEmbeddings('bert-base-multilingual-cased') bert_matcher = Embeddings(bert, min_similarity=0) fasttext = WordEmbeddings('en-crawl') fasttext_matcher = Embeddings(fasttext, min_similarity=0) matchers = [bert_matcher, fasttext_matcher] models = PolyFuzz(matchers).match(from_list, to_list)","title":"Embeddings"},{"location":"tutorial/models/models/#using-multiple-models","text":"from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance, TFIDF, Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] bert = TransformerWordEmbeddings('bert-base-multilingual-cased') bert_matcher = Embeddings(bert, min_similarity=0, matcher_id=\"BERT\") tfidf_matcher = TFIDF(min_similarity=0) edit_matcher = EditDistance() matchers = [bert_matcher, tfidf_matcher, edit_matcher] models = PolyFuzz(matchers).match(from_list, to_list) To access the results, we again can call get_matches but since we have multiple models we get back a dictionary of dataframes back. In order to access the results of a specific model, call get_matches with the correct id: >>> models.get_matches(\"BERT\") From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.928045 3 recal apples 0.825268 4 house mouse 0.887524 5 similarity mouse 0.791548 Finally, visualize the results to compare the models: models.visualize_precision_recall(kde=True)","title":"Using Multiple Models"},{"location":"tutorial/quickstart/quickstart/","text":"Installation \u00b6 You can install PolyFuzz via pip: pip install polyfuzz This will install the base dependencies and excludes any deep learning/embedding models. If you want to be making use of \ud83e\udd17 Transformers, install the additional additional Flair dependency: pip install polyfuzz[flair] Getting Started \u00b6 The main goal of PolyFuzz is to allow the user to perform different methods for matching strings. We start by defining two lists, one to map from and one to map to. We are going to be using TF-IDF to create n-grams on a character level in order to compare similarity between strings. We only have to instantiate PolyFuzz with TF-IDF and match the lists: from polyfuzz import PolyFuzz from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] model = PolyFuzz(\"TF-IDF\").match(from_list, to_list) The resulting matches can be accessed through model.get_matches() : >>> model.get_matches() From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.783751 3 recal None 0.000000 4 house mouse 0.587927 5 similarity None 0.000000 NOTE : When instantiating PolyFuzz we also could have used \"EditDistance\" or \"Embeddings\" to quickly access Levenshtein and FastText (English) respectively. Group Matches \u00b6 We can group the matches To as there might be significant overlap in strings in our to_list. To do this, we calculate the similarity within strings in to_list and use single linkage to then group the strings with a high similarity. When we extract the new matches, we can see an additional column Group in which all the To matches were grouped to: >>> model.group(link_min_similarity=0.75) >>> model.get_matches() From To Similarity Group 0 apple apple 1.000000 apples 1 apples apples 1.000000 apples 2 appl apple 0.783751 apples 3 recal None 0.000000 None 4 house mouse 0.587927 mouse 5 similarity None 0.000000 None As can be seen above, we grouped apple and apples together to apple such that when a string is mapped to apple it will fall in the cluster of [apples, apple] and will be mapped to the first instance in the cluster which is apples . Precision-Recall Curve \u00b6 Next, we would like to see how well our model is doing on our data. We express our results as precision and recall where precision is defined as the minimum similarity score before a match is correct and recall the percentage of matches found at a certain minimum similarity score. Creating the visualizations is as simple as: model.visualize_precision_recall()","title":"Quickstart"},{"location":"tutorial/quickstart/quickstart/#installation","text":"You can install PolyFuzz via pip: pip install polyfuzz This will install the base dependencies and excludes any deep learning/embedding models. If you want to be making use of \ud83e\udd17 Transformers, install the additional additional Flair dependency: pip install polyfuzz[flair]","title":"Installation"},{"location":"tutorial/quickstart/quickstart/#getting-started","text":"The main goal of PolyFuzz is to allow the user to perform different methods for matching strings. We start by defining two lists, one to map from and one to map to. We are going to be using TF-IDF to create n-grams on a character level in order to compare similarity between strings. We only have to instantiate PolyFuzz with TF-IDF and match the lists: from polyfuzz import PolyFuzz from_list = [\"apple\", \"apples\", \"appl\", \"recal\", \"house\", \"similarity\"] to_list = [\"apple\", \"apples\", \"mouse\"] model = PolyFuzz(\"TF-IDF\").match(from_list, to_list) The resulting matches can be accessed through model.get_matches() : >>> model.get_matches() From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.783751 3 recal None 0.000000 4 house mouse 0.587927 5 similarity None 0.000000 NOTE : When instantiating PolyFuzz we also could have used \"EditDistance\" or \"Embeddings\" to quickly access Levenshtein and FastText (English) respectively.","title":"Getting Started"},{"location":"tutorial/quickstart/quickstart/#group-matches","text":"We can group the matches To as there might be significant overlap in strings in our to_list. To do this, we calculate the similarity within strings in to_list and use single linkage to then group the strings with a high similarity. When we extract the new matches, we can see an additional column Group in which all the To matches were grouped to: >>> model.group(link_min_similarity=0.75) >>> model.get_matches() From To Similarity Group 0 apple apple 1.000000 apples 1 apples apples 1.000000 apples 2 appl apple 0.783751 apples 3 recal None 0.000000 None 4 house mouse 0.587927 mouse 5 similarity None 0.000000 None As can be seen above, we grouped apple and apples together to apple such that when a string is mapped to apple it will fall in the cluster of [apples, apple] and will be mapped to the first instance in the cluster which is apples .","title":"Group Matches"},{"location":"tutorial/quickstart/quickstart/#precision-recall-curve","text":"Next, we would like to see how well our model is doing on our data. We express our results as precision and recall where precision is defined as the minimum similarity score before a match is correct and recall the percentage of matches found at a certain minimum similarity score. Creating the visualizations is as simple as: model.visualize_precision_recall()","title":"Precision-Recall Curve"}]}